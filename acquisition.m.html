<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>skopt.acquisition API documentation</title>
    <meta name="description" content="" />

  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 0.9em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    padding-top: 0px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  }

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; }

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;

      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
</head>
<body>
<a href="https://github.com/scikit-optimize/scikit-optimize"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <ul id="index">
    <li class="set"><h3><a href="https://scikit-optimize.github.io/">Index</a></h3></li>


    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#skopt.acquisition.gaussian_acquisition_1D">gaussian_acquisition_1D</a></li>
    <li class="mono"><a href="#skopt.acquisition.gaussian_ei">gaussian_ei</a></li>
    <li class="mono"><a href="#skopt.acquisition.gaussian_lcb">gaussian_lcb</a></li>
    <li class="mono"><a href="#skopt.acquisition.gaussian_pi">gaussian_pi</a></li>
  </ul>

    </li>



    <li class="set"><h3><a href="https://scikit-optimize.github.io/"></a></h3>
    </li>

    <li class="set"><h3><a href="#">Notebooks</a></h3>
      <ul>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/ask-and-tell.html">Ask and tell</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html">Bayesian optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/hyperparameter-optimization.html">Hyperparameter optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/interruptible-optimization.html">Interruptible optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/parallel-optimization.html">Parallel optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html">Sklearn gridsearchcv replacement</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/store-and-load-results.html">Store and load results</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/strategy-comparison.html">Strategy comparison</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/visualizing-results.html">Visualizing results</a></li>
      </ul>
    </li>
    </ul>
  </div>

    <article id="content">
          
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">skopt.acquisition</span> module</h1>
  
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.acquisition', this);">Show source &equiv;</a></p>
  <div id="source-skopt.acquisition" class="source">
    <div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>


<span class="k">def</span> <span class="nf">gaussian_acquisition_1D</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;LCB&quot;</span><span class="p">,</span>
                            <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper around the acquisition function that is called by fmin_l_bfgs_b.</span>

<span class="sd">    This is because lbfgs allows only 1-D input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_gaussian_acquisition</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                 <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
                                 <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="n">acq_func_kwargs</span><span class="p">,</span>
                                 <span class="n">return_grad</span><span class="o">=</span><span class="n">return_grad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_gaussian_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;LCB&quot;</span><span class="p">,</span>
                          <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper so that the output of this function can be</span>
<span class="sd">    directly passed to a minimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check inputs</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X is </span><span class="si">{}</span><span class="s2">-dimensional, however,&quot;</span>
                         <span class="s2">&quot; it must be 2-dimensional.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">acq_func_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">acq_func_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">acq_func_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;xi&quot;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">acq_func_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;kappa&quot;</span><span class="p">,</span> <span class="mf">1.96</span><span class="p">)</span>

    <span class="c1"># Evaluate acquisition function</span>
    <span class="n">per_second</span> <span class="o">=</span> <span class="n">acq_func</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;ps&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">per_second</span><span class="p">:</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">time_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">estimators_</span>

    <span class="k">if</span> <span class="n">acq_func</span> <span class="o">==</span> <span class="s2">&quot;LCB&quot;</span><span class="p">:</span>
        <span class="n">func_and_grad</span> <span class="o">=</span> <span class="n">gaussian_lcb</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">kappa</span><span class="p">,</span> <span class="n">return_grad</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">acq_vals</span><span class="p">,</span> <span class="n">acq_grad</span> <span class="o">=</span> <span class="n">func_and_grad</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">acq_vals</span> <span class="o">=</span> <span class="n">func_and_grad</span>

    <span class="k">elif</span> <span class="n">acq_func</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="s2">&quot;PI&quot;</span><span class="p">,</span> <span class="s2">&quot;EIps&quot;</span><span class="p">,</span> <span class="s2">&quot;PIps&quot;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">acq_func</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="s2">&quot;EIps&quot;</span><span class="p">]:</span>
            <span class="n">func_and_grad</span> <span class="o">=</span> <span class="n">gaussian_ei</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">return_grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func_and_grad</span> <span class="o">=</span> <span class="n">gaussian_pi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">return_grad</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">acq_vals</span> <span class="o">=</span> <span class="o">-</span><span class="n">func_and_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">acq_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">func_and_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">acq_vals</span> <span class="o">=</span> <span class="o">-</span><span class="n">func_and_grad</span>

        <span class="k">if</span> <span class="n">acq_func</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;EIps&quot;</span><span class="p">,</span> <span class="s2">&quot;PIps&quot;</span><span class="p">]:</span>

            <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
                <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">time_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">time_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># acq = acq / E(t)</span>
            <span class="n">inv_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">mu</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">std</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">acq_vals</span> <span class="o">*=</span> <span class="n">inv_t</span>

            <span class="c1"># grad = d(acq_func) * inv_t + (acq_vals *d(inv_t))</span>
            <span class="c1"># inv_t = exp(g)</span>
            <span class="c1"># d(inv_t) = inv_t * grad(g)</span>
            <span class="c1"># d(inv_t) = inv_t * (-mu_grad + std * std_grad)</span>
            <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
                <span class="n">acq_grad</span> <span class="o">*=</span> <span class="n">inv_t</span>
                <span class="n">acq_grad</span> <span class="o">+=</span> <span class="n">acq_vals</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">mu_grad</span> <span class="o">+</span> <span class="n">std</span><span class="o">*</span><span class="n">std_grad</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Acquisition function not implemented.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">acq_vals</span><span class="p">,</span> <span class="n">acq_grad</span>
    <span class="k">return</span> <span class="n">acq_vals</span>


<span class="k">def</span> <span class="nf">gaussian_lcb</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use the lower confidence bound to estimate the acquisition</span>
<span class="sd">    values.</span>

<span class="sd">    The trade-off between exploitation and exploration is left to</span>
<span class="sd">    be controlled by the user through the parameter ``kappa``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Values where the acquisition function should be computed.</span>

<span class="sd">    * `model` [sklearn estimator that implements predict with ``return_std``]:</span>
<span class="sd">        The fit estimator that approximates the function through the</span>
<span class="sd">        method ``predict``.</span>
<span class="sd">        It should have a ``return_std`` parameter that returns the standard</span>
<span class="sd">        deviation.</span>

<span class="sd">    * `kappa`: [float, default 1.96 or &#39;inf&#39;]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        If set to &#39;inf&#39;, the acquisition function will only use the variance</span>
<span class="sd">        which is useful in a pure exploration setting.</span>
<span class="sd">        Useless if ``method`` is set to &quot;LCB&quot;.</span>

<span class="sd">    * `return_grad`: [boolean, optional]:</span>
<span class="sd">        Whether or not to return the grad. Implemented only for the case where</span>
<span class="sd">        ``X`` is a single sample.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `values`: [array-like, shape=(X.shape[0],)]:</span>
<span class="sd">        Acquisition function values computed at X.</span>

<span class="sd">    * `grad`: [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Gradient at X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute posterior.</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">kappa</span> <span class="o">==</span> <span class="s2">&quot;inf&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="n">std</span><span class="p">,</span> <span class="o">-</span><span class="n">std_grad</span>
            <span class="k">return</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">std_grad</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kappa</span> <span class="o">==</span> <span class="s2">&quot;inf&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="n">std</span>
            <span class="k">return</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">std</span>


<span class="k">def</span> <span class="nf">gaussian_pi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use the probability of improvement to calculate the acquisition values.</span>

<span class="sd">    The conditional probability `P(y=f(x) | x)`form a gaussian with a</span>
<span class="sd">    certain mean and standard deviation approximated by the model.</span>

<span class="sd">    The PI condition is derived by computing ``E[u(f(x))]``</span>
<span class="sd">    where ``u(f(x)) = 1``, if ``f(x) &lt; y_opt`` and ``u(f(x)) = 0``,</span>
<span class="sd">    if``f(x) &gt; y_opt``.</span>

<span class="sd">    This means that the PI condition does not care about how &quot;better&quot; the</span>
<span class="sd">    predictions are than the previous values, since it gives an equal reward</span>
<span class="sd">    to all of them.</span>

<span class="sd">    Note that the value returned by this function should be maximized to</span>
<span class="sd">    obtain the ``X`` with maximum improvement.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Values where the acquisition function should be computed.</span>

<span class="sd">    * `model` [sklearn estimator that implements predict with ``return_std``]:</span>
<span class="sd">        The fit estimator that approximates the function through the</span>
<span class="sd">        method ``predict``.</span>
<span class="sd">        It should have a ``return_std`` parameter that returns the standard</span>
<span class="sd">        deviation.</span>

<span class="sd">    * `y_opt` [float, default 0]:</span>
<span class="sd">        Previous minimum value which we would like to improve upon.</span>

<span class="sd">    * `xi`: [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Useful only when ``method`` is set to &quot;EI&quot;</span>

<span class="sd">    * `return_grad`: [boolean, optional]:</span>
<span class="sd">        Whether or not to return the grad. Implemented only for the case where</span>
<span class="sd">        ``X`` is a single sample.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `values`: [array-like, shape=(X.shape[0],)]:</span>
<span class="sd">        Acquisition function values computed at X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># check dimensionality of mu, std so we can divide them below</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">std</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mu and std are </span><span class="si">{}</span><span class="s2">-dimensional and </span><span class="si">{}</span><span class="s2">-dimensional, &quot;</span>
                         <span class="s2">&quot;however both must be 1-dimensional. Did you train &quot;</span>
                         <span class="s2">&quot;your model with an (N, 1) vector instead of an &quot;</span>
                         <span class="s2">&quot;(N,) vector?&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">std</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">std</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">improve</span> <span class="o">=</span> <span class="n">y_opt</span> <span class="o">-</span> <span class="n">xi</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">improve</span> <span class="o">/</span> <span class="n">std</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">values</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">std_grad</span><span class="p">)</span>

        <span class="c1"># Substitute (y_opt - xi - mu) / sigma = t and apply chain rule.</span>
        <span class="c1"># improve_grad is the gradient of t wrt x.</span>
        <span class="n">improve_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu_grad</span> <span class="o">*</span> <span class="n">std</span> <span class="o">-</span> <span class="n">std_grad</span> <span class="o">*</span> <span class="n">improve</span>
        <span class="n">improve_grad</span> <span class="o">/=</span> <span class="n">std</span><span class="o">**</span><span class="mi">2</span>

        <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">improve_grad</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">values</span>


<span class="k">def</span> <span class="nf">gaussian_ei</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use the expected improvement to calculate the acquisition values.</span>

<span class="sd">    The conditional probability `P(y=f(x) | x)`form a gaussian with a certain</span>
<span class="sd">    mean and standard deviation approximated by the model.</span>

<span class="sd">    The EI condition is derived by computing ``E[u(f(x))]``</span>
<span class="sd">    where ``u(f(x)) = 0``, if ``f(x) &gt; y_opt`` and ``u(f(x)) = y_opt - f(x)``,</span>
<span class="sd">    if``f(x) &lt; y_opt``.</span>

<span class="sd">    This solves one of the issues of the PI condition by giving a reward</span>
<span class="sd">    proportional to the amount of improvement got.</span>

<span class="sd">    Note that the value returned by this function should be maximized to</span>
<span class="sd">    obtain the ``X`` with maximum improvement.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Values where the acquisition function should be computed.</span>

<span class="sd">    * `model` [sklearn estimator that implements predict with ``return_std``]:</span>
<span class="sd">        The fit estimator that approximates the function through the</span>
<span class="sd">        method ``predict``.</span>
<span class="sd">        It should have a ``return_std`` parameter that returns the standard</span>
<span class="sd">        deviation.</span>

<span class="sd">    * `y_opt` [float, default 0]:</span>
<span class="sd">        Previous minimum value which we would like to improve upon.</span>

<span class="sd">    * `xi`: [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Useful only when ``method`` is set to &quot;EI&quot;</span>

<span class="sd">    * `return_grad`: [boolean, optional]:</span>
<span class="sd">        Whether or not to return the grad. Implemented only for the case where</span>
<span class="sd">        ``X`` is a single sample.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `values`: [array-like, shape=(X.shape[0],)]:</span>
<span class="sd">        Acquisition function values computed at X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># check dimensionality of mu, std so we can divide them below</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">std</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mu and std are </span><span class="si">{}</span><span class="s2">-dimensional and </span><span class="si">{}</span><span class="s2">-dimensional, &quot;</span>
                         <span class="s2">&quot;however both must be 1-dimensional. Did you train &quot;</span>
                         <span class="s2">&quot;your model with an (N, 1) vector instead of an &quot;</span>
                         <span class="s2">&quot;(N,) vector?&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">std</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">std</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">improve</span> <span class="o">=</span> <span class="n">y_opt</span> <span class="o">-</span> <span class="n">xi</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">improve</span> <span class="o">/</span> <span class="n">std</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>
    <span class="n">exploit</span> <span class="o">=</span> <span class="n">improve</span> <span class="o">*</span> <span class="n">cdf</span>
    <span class="n">explore</span> <span class="o">=</span> <span class="n">std</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">pdf</span>
    <span class="n">values</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">exploit</span> <span class="o">+</span> <span class="n">explore</span>

    <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">std_grad</span><span class="p">)</span>

        <span class="c1"># Substitute (y_opt - xi - mu) / sigma = t and apply chain rule.</span>
        <span class="c1"># improve_grad is the gradient of t wrt x.</span>
        <span class="n">improve_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu_grad</span> <span class="o">*</span> <span class="n">std</span> <span class="o">-</span> <span class="n">std_grad</span> <span class="o">*</span> <span class="n">improve</span>
        <span class="n">improve_grad</span> <span class="o">/=</span> <span class="n">std</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">cdf_grad</span> <span class="o">=</span> <span class="n">improve_grad</span> <span class="o">*</span> <span class="n">pdf</span>
        <span class="n">pdf_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">improve</span> <span class="o">*</span> <span class="n">cdf_grad</span>
        <span class="n">exploit_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu_grad</span> <span class="o">*</span> <span class="n">cdf</span> <span class="o">-</span> <span class="n">pdf_grad</span>
        <span class="n">explore_grad</span> <span class="o">=</span> <span class="n">std_grad</span> <span class="o">*</span> <span class="n">pdf</span> <span class="o">+</span> <span class="n">pdf_grad</span>

        <span class="n">grad</span> <span class="o">=</span> <span class="n">exploit_grad</span> <span class="o">+</span> <span class="n">explore_grad</span>
        <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad</span>

    <span class="k">return</span> <span class="n">values</span>
</pre></div>

  </div>

  </header>



  <section id="section-items">

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="skopt.acquisition.gaussian_acquisition_1D">
    <p>def <span class="ident">gaussian_acquisition_1D</span>(</p><p>X, model, y_opt=None, acq_func=&#39;LCB&#39;, acq_func_kwargs=None, return_grad=True)</p>
    </div>
    

    
  
    <div class="desc"><p>A wrapper around the acquisition function that is called by fmin_l_bfgs_b.</p>
<p>This is because lbfgs allows only 1-D input.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.acquisition.gaussian_acquisition_1D', this);">Show source &equiv;</a></p>
  <div id="source-skopt.acquisition.gaussian_acquisition_1D" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gaussian_acquisition_1D</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;LCB&quot;</span><span class="p">,</span>
                            <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper around the acquisition function that is called by fmin_l_bfgs_b.</span>

<span class="sd">    This is because lbfgs allows only 1-D input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_gaussian_acquisition</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                 <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
                                 <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="n">acq_func_kwargs</span><span class="p">,</span>
                                 <span class="n">return_grad</span><span class="o">=</span><span class="n">return_grad</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.acquisition.gaussian_ei">
    <p>def <span class="ident">gaussian_ei</span>(</p><p>X, model, y_opt=0.0, xi=0.01, return_grad=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Use the expected improvement to calculate the acquisition values.</p>
<p>The conditional probability <code>P(y=f(x) | x)</code>form a gaussian with a certain
mean and standard deviation approximated by the model.</p>
<p>The EI condition is derived by computing <code>E[u(f(x))]</code>
where <code>u(f(x)) = 0</code>, if <code>f(x) &gt; y_opt</code> and <code>u(f(x)) = y_opt - f(x)</code>,
if<code>f(x) &lt; y_opt</code>.</p>
<p>This solves one of the issues of the PI condition by giving a reward
proportional to the amount of improvement got.</p>
<p>Note that the value returned by this function should be maximized to
obtain the <code>X</code> with maximum improvement.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Values where the acquisition function should be computed.</p>
</li>
<li>
<p><code>model</code> [sklearn estimator that implements predict with <code>return_std</code>]:
    The fit estimator that approximates the function through the
    method <code>predict</code>.
    It should have a <code>return_std</code> parameter that returns the standard
    deviation.</p>
</li>
<li>
<p><code>y_opt</code> [float, default 0]:
    Previous minimum value which we would like to improve upon.</p>
</li>
<li>
<p><code>xi</code>: [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Useful only when <code>method</code> is set to "EI"</p>
</li>
<li>
<p><code>return_grad</code>: [boolean, optional]:
    Whether or not to return the grad. Implemented only for the case where
    <code>X</code> is a single sample.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>values</code>: [array-like, shape=(X.shape[0],)]:
    Acquisition function values computed at X.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.acquisition.gaussian_ei', this);">Show source &equiv;</a></p>
  <div id="source-skopt.acquisition.gaussian_ei" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gaussian_ei</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use the expected improvement to calculate the acquisition values.</span>

<span class="sd">    The conditional probability `P(y=f(x) | x)`form a gaussian with a certain</span>
<span class="sd">    mean and standard deviation approximated by the model.</span>

<span class="sd">    The EI condition is derived by computing ``E[u(f(x))]``</span>
<span class="sd">    where ``u(f(x)) = 0``, if ``f(x) &gt; y_opt`` and ``u(f(x)) = y_opt - f(x)``,</span>
<span class="sd">    if``f(x) &lt; y_opt``.</span>

<span class="sd">    This solves one of the issues of the PI condition by giving a reward</span>
<span class="sd">    proportional to the amount of improvement got.</span>

<span class="sd">    Note that the value returned by this function should be maximized to</span>
<span class="sd">    obtain the ``X`` with maximum improvement.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Values where the acquisition function should be computed.</span>

<span class="sd">    * `model` [sklearn estimator that implements predict with ``return_std``]:</span>
<span class="sd">        The fit estimator that approximates the function through the</span>
<span class="sd">        method ``predict``.</span>
<span class="sd">        It should have a ``return_std`` parameter that returns the standard</span>
<span class="sd">        deviation.</span>

<span class="sd">    * `y_opt` [float, default 0]:</span>
<span class="sd">        Previous minimum value which we would like to improve upon.</span>

<span class="sd">    * `xi`: [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Useful only when ``method`` is set to &quot;EI&quot;</span>

<span class="sd">    * `return_grad`: [boolean, optional]:</span>
<span class="sd">        Whether or not to return the grad. Implemented only for the case where</span>
<span class="sd">        ``X`` is a single sample.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `values`: [array-like, shape=(X.shape[0],)]:</span>
<span class="sd">        Acquisition function values computed at X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># check dimensionality of mu, std so we can divide them below</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">std</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mu and std are </span><span class="si">{}</span><span class="s2">-dimensional and </span><span class="si">{}</span><span class="s2">-dimensional, &quot;</span>
                         <span class="s2">&quot;however both must be 1-dimensional. Did you train &quot;</span>
                         <span class="s2">&quot;your model with an (N, 1) vector instead of an &quot;</span>
                         <span class="s2">&quot;(N,) vector?&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">std</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">std</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">improve</span> <span class="o">=</span> <span class="n">y_opt</span> <span class="o">-</span> <span class="n">xi</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">improve</span> <span class="o">/</span> <span class="n">std</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>
    <span class="n">exploit</span> <span class="o">=</span> <span class="n">improve</span> <span class="o">*</span> <span class="n">cdf</span>
    <span class="n">explore</span> <span class="o">=</span> <span class="n">std</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">pdf</span>
    <span class="n">values</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">exploit</span> <span class="o">+</span> <span class="n">explore</span>

    <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">std_grad</span><span class="p">)</span>

        <span class="c1"># Substitute (y_opt - xi - mu) / sigma = t and apply chain rule.</span>
        <span class="c1"># improve_grad is the gradient of t wrt x.</span>
        <span class="n">improve_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu_grad</span> <span class="o">*</span> <span class="n">std</span> <span class="o">-</span> <span class="n">std_grad</span> <span class="o">*</span> <span class="n">improve</span>
        <span class="n">improve_grad</span> <span class="o">/=</span> <span class="n">std</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">cdf_grad</span> <span class="o">=</span> <span class="n">improve_grad</span> <span class="o">*</span> <span class="n">pdf</span>
        <span class="n">pdf_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">improve</span> <span class="o">*</span> <span class="n">cdf_grad</span>
        <span class="n">exploit_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu_grad</span> <span class="o">*</span> <span class="n">cdf</span> <span class="o">-</span> <span class="n">pdf_grad</span>
        <span class="n">explore_grad</span> <span class="o">=</span> <span class="n">std_grad</span> <span class="o">*</span> <span class="n">pdf</span> <span class="o">+</span> <span class="n">pdf_grad</span>

        <span class="n">grad</span> <span class="o">=</span> <span class="n">exploit_grad</span> <span class="o">+</span> <span class="n">explore_grad</span>
        <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad</span>

    <span class="k">return</span> <span class="n">values</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.acquisition.gaussian_lcb">
    <p>def <span class="ident">gaussian_lcb</span>(</p><p>X, model, kappa=1.96, return_grad=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Use the lower confidence bound to estimate the acquisition
values.</p>
<p>The trade-off between exploitation and exploration is left to
be controlled by the user through the parameter <code>kappa</code>.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Values where the acquisition function should be computed.</p>
</li>
<li>
<p><code>model</code> [sklearn estimator that implements predict with <code>return_std</code>]:
    The fit estimator that approximates the function through the
    method <code>predict</code>.
    It should have a <code>return_std</code> parameter that returns the standard
    deviation.</p>
</li>
<li>
<p><code>kappa</code>: [float, default 1.96 or 'inf']:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    If set to 'inf', the acquisition function will only use the variance
    which is useful in a pure exploration setting.
    Useless if <code>method</code> is set to "LCB".</p>
</li>
<li>
<p><code>return_grad</code>: [boolean, optional]:
    Whether or not to return the grad. Implemented only for the case where
    <code>X</code> is a single sample.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>values</code>: [array-like, shape=(X.shape[0],)]:
    Acquisition function values computed at X.</p>
</li>
<li>
<p><code>grad</code>: [array-like, shape=(n_samples, n_features)]:
    Gradient at X.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.acquisition.gaussian_lcb', this);">Show source &equiv;</a></p>
  <div id="source-skopt.acquisition.gaussian_lcb" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gaussian_lcb</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use the lower confidence bound to estimate the acquisition</span>
<span class="sd">    values.</span>

<span class="sd">    The trade-off between exploitation and exploration is left to</span>
<span class="sd">    be controlled by the user through the parameter ``kappa``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Values where the acquisition function should be computed.</span>

<span class="sd">    * `model` [sklearn estimator that implements predict with ``return_std``]:</span>
<span class="sd">        The fit estimator that approximates the function through the</span>
<span class="sd">        method ``predict``.</span>
<span class="sd">        It should have a ``return_std`` parameter that returns the standard</span>
<span class="sd">        deviation.</span>

<span class="sd">    * `kappa`: [float, default 1.96 or &#39;inf&#39;]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        If set to &#39;inf&#39;, the acquisition function will only use the variance</span>
<span class="sd">        which is useful in a pure exploration setting.</span>
<span class="sd">        Useless if ``method`` is set to &quot;LCB&quot;.</span>

<span class="sd">    * `return_grad`: [boolean, optional]:</span>
<span class="sd">        Whether or not to return the grad. Implemented only for the case where</span>
<span class="sd">        ``X`` is a single sample.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `values`: [array-like, shape=(X.shape[0],)]:</span>
<span class="sd">        Acquisition function values computed at X.</span>

<span class="sd">    * `grad`: [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Gradient at X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute posterior.</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">kappa</span> <span class="o">==</span> <span class="s2">&quot;inf&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="n">std</span><span class="p">,</span> <span class="o">-</span><span class="n">std_grad</span>
            <span class="k">return</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">std_grad</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kappa</span> <span class="o">==</span> <span class="s2">&quot;inf&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="n">std</span>
            <span class="k">return</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">std</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.acquisition.gaussian_pi">
    <p>def <span class="ident">gaussian_pi</span>(</p><p>X, model, y_opt=0.0, xi=0.01, return_grad=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Use the probability of improvement to calculate the acquisition values.</p>
<p>The conditional probability <code>P(y=f(x) | x)</code>form a gaussian with a
certain mean and standard deviation approximated by the model.</p>
<p>The PI condition is derived by computing <code>E[u(f(x))]</code>
where <code>u(f(x)) = 1</code>, if <code>f(x) &lt; y_opt</code> and <code>u(f(x)) = 0</code>,
if<code>f(x) &gt; y_opt</code>.</p>
<p>This means that the PI condition does not care about how "better" the
predictions are than the previous values, since it gives an equal reward
to all of them.</p>
<p>Note that the value returned by this function should be maximized to
obtain the <code>X</code> with maximum improvement.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Values where the acquisition function should be computed.</p>
</li>
<li>
<p><code>model</code> [sklearn estimator that implements predict with <code>return_std</code>]:
    The fit estimator that approximates the function through the
    method <code>predict</code>.
    It should have a <code>return_std</code> parameter that returns the standard
    deviation.</p>
</li>
<li>
<p><code>y_opt</code> [float, default 0]:
    Previous minimum value which we would like to improve upon.</p>
</li>
<li>
<p><code>xi</code>: [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Useful only when <code>method</code> is set to "EI"</p>
</li>
<li>
<p><code>return_grad</code>: [boolean, optional]:
    Whether or not to return the grad. Implemented only for the case where
    <code>X</code> is a single sample.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>values</code>: [array-like, shape=(X.shape[0],)]:
    Acquisition function values computed at X.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.acquisition.gaussian_pi', this);">Show source &equiv;</a></p>
  <div id="source-skopt.acquisition.gaussian_pi" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gaussian_pi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use the probability of improvement to calculate the acquisition values.</span>

<span class="sd">    The conditional probability `P(y=f(x) | x)`form a gaussian with a</span>
<span class="sd">    certain mean and standard deviation approximated by the model.</span>

<span class="sd">    The PI condition is derived by computing ``E[u(f(x))]``</span>
<span class="sd">    where ``u(f(x)) = 1``, if ``f(x) &lt; y_opt`` and ``u(f(x)) = 0``,</span>
<span class="sd">    if``f(x) &gt; y_opt``.</span>

<span class="sd">    This means that the PI condition does not care about how &quot;better&quot; the</span>
<span class="sd">    predictions are than the previous values, since it gives an equal reward</span>
<span class="sd">    to all of them.</span>

<span class="sd">    Note that the value returned by this function should be maximized to</span>
<span class="sd">    obtain the ``X`` with maximum improvement.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Values where the acquisition function should be computed.</span>

<span class="sd">    * `model` [sklearn estimator that implements predict with ``return_std``]:</span>
<span class="sd">        The fit estimator that approximates the function through the</span>
<span class="sd">        method ``predict``.</span>
<span class="sd">        It should have a ``return_std`` parameter that returns the standard</span>
<span class="sd">        deviation.</span>

<span class="sd">    * `y_opt` [float, default 0]:</span>
<span class="sd">        Previous minimum value which we would like to improve upon.</span>

<span class="sd">    * `xi`: [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Useful only when ``method`` is set to &quot;EI&quot;</span>

<span class="sd">    * `return_grad`: [boolean, optional]:</span>
<span class="sd">        Whether or not to return the grad. Implemented only for the case where</span>
<span class="sd">        ``X`` is a single sample.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `values`: [array-like, shape=(X.shape[0],)]:</span>
<span class="sd">        Acquisition function values computed at X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">mu_grad</span><span class="p">,</span> <span class="n">std_grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_mean_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_std_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># check dimensionality of mu, std so we can divide them below</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">std</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mu and std are </span><span class="si">{}</span><span class="s2">-dimensional and </span><span class="si">{}</span><span class="s2">-dimensional, &quot;</span>
                         <span class="s2">&quot;however both must be 1-dimensional. Did you train &quot;</span>
                         <span class="s2">&quot;your model with an (N, 1) vector instead of an &quot;</span>
                         <span class="s2">&quot;(N,) vector?&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">std</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">std</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">improve</span> <span class="o">=</span> <span class="n">y_opt</span> <span class="o">-</span> <span class="n">xi</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">improve</span> <span class="o">/</span> <span class="n">std</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">values</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_grad</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">std_grad</span><span class="p">)</span>

        <span class="c1"># Substitute (y_opt - xi - mu) / sigma = t and apply chain rule.</span>
        <span class="c1"># improve_grad is the gradient of t wrt x.</span>
        <span class="n">improve_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu_grad</span> <span class="o">*</span> <span class="n">std</span> <span class="o">-</span> <span class="n">std_grad</span> <span class="o">*</span> <span class="n">improve</span>
        <span class="n">improve_grad</span> <span class="o">/=</span> <span class="n">std</span><span class="o">**</span><span class="mi">2</span>

        <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">improve_grad</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">values</span>
</pre></div>

  </div>
</div>

  </div>
  

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
