<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>skopt API documentation</title>
    <meta name="description" content="Scikit-Optimize, or `skopt`, is a simple and efficient library to
minimize (very) expensive and nois..." />

  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 0.9em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    padding-top: 0px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  }

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; }

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;

      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
</head>
<body>
<a href="https://github.com/scikit-optimize/scikit-optimize"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <ul id="index">
    <li class="set"><h3><a href="https://scikit-optimize.github.io/">Index</a></h3></li>


    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#skopt.dummy_minimize">dummy_minimize</a></li>
    <li class="mono"><a href="#skopt.dump">dump</a></li>
    <li class="mono"><a href="#skopt.expected_minimum">expected_minimum</a></li>
    <li class="mono"><a href="#skopt.forest_minimize">forest_minimize</a></li>
    <li class="mono"><a href="#skopt.gbrt_minimize">gbrt_minimize</a></li>
    <li class="mono"><a href="#skopt.gp_minimize">gp_minimize</a></li>
    <li class="mono"><a href="#skopt.load">load</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#skopt.BayesSearchCV">BayesSearchCV</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.Optimizer">Optimizer</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.Space">Space</a></span>
        </li>
      </ul>
    </li>

    <li class="set"><h3><a href="#header-submodules">Sub-modules</a></h3>
      <ul>
        <li class="mono"><a href="acquisition.m.html">skopt.acquisition</a></li>
        <li class="mono"><a href="benchmarks.m.html">skopt.benchmarks</a></li>
        <li class="mono"><a href="callbacks.m.html">skopt.callbacks</a></li>
        <li class="mono"><a href="learning/index.html">skopt.learning</a></li>
        <li class="mono"><a href="optimizer/index.html">skopt.optimizer</a></li>
        <li class="mono"><a href="plots.m.html">skopt.plots</a></li>
        <li class="mono"><a href="space/index.html">skopt.space</a></li>
      </ul>
    </li>


    <li class="set"><h3><a href="#">Notebooks</a></h3>
      <ul>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/ask-and-tell.html">Ask and tell</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html">Bayesian optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/hyperparameter-optimization.html">Hyperparameter optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/parallel-optimization.html">Parallel optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html">Sklearn gridsearchcv replacement</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/store-and-load-results.html">Store and load results</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/strategy-comparison.html">Strategy comparison</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/visualizing-results.html">Visualizing results</a></li>
      </ul>
    </li>
    </ul>
  </div>

    <article id="content">
          
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">skopt</span> module</h1>
  <p>Scikit-Optimize, or <code>skopt</code>, is a simple and efficient library to
minimize (very) expensive and noisy black-box functions. It implements
several methods for sequential model-based optimization. <code>skopt</code> is reusable
in many contexts and accessible.</p>
<p><a href="https://travis-ci.org/scikit-optimize/scikit-optimize"><img alt="Build Status" src="https://travis-ci.org/scikit-optimize/scikit-optimize.svg?branch=master" /></a></p>
<h2>Install</h2>
<div class="codehilite"><pre><span></span>pip install scikit-optimize
</pre></div>


<h2>Getting started</h2>
<p>Find the minimum of the noisy function <code>f(x)</code> over the range <code>-2 &lt; x &lt; 2</code>
with <code>skopt</code>:</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">gp_minimize</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">gp_minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)])</span>
</pre></div>


<p>For more read our <a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html">introduction to bayesian optimization</a>
and the other <a href="https://github.com/scikit-optimize/scikit-optimize/tree/master/examples">examples</a>.</p>
<h2>Development</h2>
<p>The library is still experimental and under heavy development.</p>
<p>The development version can be installed through:</p>
<div class="codehilite"><pre><span></span>git clone https://github.com/scikit-optimize/scikit-optimize.git
cd scikit-optimize
pip install -r requirements.txt
python setup.py develop
</pre></div>


<p>Run the tests by executing <code>pytest</code> in the top level directory.</p>
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt', this);">Show source &equiv;</a></p>
  <div id="source-skopt" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Scikit-Optimize, or `skopt`, is a simple and efficient library to</span>
<span class="sd">minimize (very) expensive and noisy black-box functions. It implements</span>
<span class="sd">several methods for sequential model-based optimization. `skopt` is reusable</span>
<span class="sd">in many contexts and accessible.</span>

<span class="sd">[![Build Status](https://travis-ci.org/scikit-optimize/scikit-optimize.svg?branch=master)](https://travis-ci.org/scikit-optimize/scikit-optimize)</span>

<span class="sd">## Install</span>

<span class="sd">```</span>
<span class="sd">pip install scikit-optimize</span>
<span class="sd">```</span>

<span class="sd">## Getting started</span>

<span class="sd">Find the minimum of the noisy function `f(x)` over the range `-2 &lt; x &lt; 2`</span>
<span class="sd">with `skopt`:</span>

<span class="sd">```python</span>
<span class="sd">import numpy as np</span>
<span class="sd">from skopt import gp_minimize</span>

<span class="sd">def f(x):</span>
<span class="sd">    return (np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2)) *</span>
<span class="sd">            np.random.randn() * 0.1)</span>

<span class="sd">res = gp_minimize(f, [(-2.0, 2.0)])</span>
<span class="sd">```</span>

<span class="sd">For more read our [introduction to bayesian optimization](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html)</span>
<span class="sd">and the other [examples](https://github.com/scikit-optimize/scikit-optimize/tree/master/examples).</span>


<span class="sd">## Development</span>

<span class="sd">The library is still experimental and under heavy development.</span>

<span class="sd">The development version can be installed through:</span>

<span class="sd">    git clone https://github.com/scikit-optimize/scikit-optimize.git</span>
<span class="sd">    cd scikit-optimize</span>
<span class="sd">    pip install -r requirements.txt</span>
<span class="sd">    python setup.py develop</span>

<span class="sd">Run the tests by executing `pytest` in the top level directory.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">acquisition</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">benchmarks</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">callbacks</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">learning</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">optimizer</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">space</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">dummy_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">forest_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">gbrt_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">gp_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">.searchcv</span> <span class="kn">import</span> <span class="n">BayesSearchCV</span>
<span class="kn">from</span> <span class="nn">.space</span> <span class="kn">import</span> <span class="n">Space</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">dump</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">expected_minimum</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">load</span>

<span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;0.5.1&quot;</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;acquisition&quot;</span><span class="p">,</span>
    <span class="s2">&quot;benchmarks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;callbacks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;learning&quot;</span><span class="p">,</span>
    <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;plots&quot;</span><span class="p">,</span>
    <span class="s2">&quot;space&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gp_minimize&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dummy_minimize&quot;</span><span class="p">,</span>
    <span class="s2">&quot;forest_minimize&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gbrt_minimize&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Optimizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dump&quot;</span><span class="p">,</span>
    <span class="s2">&quot;load&quot;</span><span class="p">,</span>
    <span class="s2">&quot;expected_minimum&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BayesSearchCV&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Space&quot;</span>
<span class="p">)</span>
</pre></div>

  </div>

  </header>



  <section id="section-items">

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="skopt.dummy_minimize">
    <p>def <span class="ident">dummy_minimize</span>(</p><p>func, dimensions, n_calls=100, x0=None, y0=None, random_state=None, verbose=False, callback=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Random search by uniform sampling within the given bounds.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a single list of parameters
    and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <code>skopt.utils.use_named_args</code> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <code>use_named_args</code> for an example.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(lower_bound, upper_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(lower_bound, upper_bound, prior)</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code> to find the minimum.</p>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]:
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, list of callables, optional]
    If callable then <code>callback(res)</code> is called after each call to <code>func</code>.
    If list of callables, then each callable in the list is called.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimisation space.</li>
<li><code>specs</code> [dict]: the call specifications.</li>
<li><code>rng</code> [RandomState instance]: State of the random state
   at the end of minimization.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.dummy_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.dummy_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">dummy_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                   <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Random search by uniform sampling within the given bounds.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a single list of parameters</span>
<span class="sd">        and return the objective value.</span>
<span class="sd">    </span>
<span class="sd">        If you have a search-space where all dimensions have names,</span>
<span class="sd">        then you can use `skopt.utils.use_named_args` as a decorator</span>
<span class="sd">        on your objective function, in order to call it directly</span>
<span class="sd">        with the named arguments. See `use_named_args` for an example.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(lower_bound, upper_bound, prior)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func` to find the minimum.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]:</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, list of callables, optional]</span>
<span class="sd">        If callable then `callback(res)` is called after each call to `func`.</span>
<span class="sd">        If list of callables, then each callable in the list is called.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimisation space.</span>
<span class="sd">        - `specs` [dict]: the call specifications.</span>
<span class="sd">        - `rng` [RandomState instance]: State of the random state</span>
<span class="sd">           at the end of minimization.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># all our calls want random suggestions, except if we need to evaluate</span>
    <span class="c1"># some initial points</span>
    <span class="k">if</span> <span class="n">x0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">y0</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">n_random_calls</span> <span class="o">=</span> <span class="n">n_calls</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_random_calls</span> <span class="o">=</span> <span class="n">n_calls</span>

    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="s2">&quot;dummy&quot;</span><span class="p">,</span>
                         <span class="c1"># explicitly set optimizer to sampling as &quot;dummy&quot;</span>
                         <span class="c1"># minimizer does not provide gradients.</span>
                         <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;sampling&quot;</span><span class="p">,</span>
                         <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_calls</span><span class="p">,</span>
                         <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                         <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                         <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.dump">
    <p>def <span class="ident">dump</span>(</p><p>res, filename, store_objective=True, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Store an skopt optimization result into a file.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    Optimization result object to be stored.</p>
</li>
<li>
<p><code>filename</code> [string or <code>pathlib.Path</code>]:
    The path of the file in which it is to be stored. The compression
    method corresponding to one of the supported filename extensions ('.z',
    '.gz', '.bz2', '.xz' or '.lzma') will be used automatically.</p>
</li>
<li>
<p><code>store_objective</code> [boolean, default=True]:
    Whether the objective function should be stored. Set <code>store_objective</code>
    to <code>False</code> if your objective function (<code>.specs['args']['func']</code>) is
    unserializable (i.e. if an exception is raised when trying to serialize
    the optimization result).</p>
<p>Notice that if <code>store_objective</code> is set to <code>False</code>, a deep copy of the
optimization result is created, potentially leading to performance
problems if <code>res</code> is very large. If the objective function is not
critical, one can delete it before calling <code>skopt.dump()</code> and thus
avoid deep copying of <code>res</code>.</p>
</li>
<li>
<p><code>**kwargs</code> [other keyword arguments]:
    All other keyword arguments will be passed to <code>joblib.dump</code>.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.dump', this);">Show source &equiv;</a></p>
  <div id="source-skopt.dump" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">dump</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">store_objective</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Store an skopt optimization result into a file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        Optimization result object to be stored.</span>

<span class="sd">    * `filename` [string or `pathlib.Path`]:</span>
<span class="sd">        The path of the file in which it is to be stored. The compression</span>
<span class="sd">        method corresponding to one of the supported filename extensions (&#39;.z&#39;,</span>
<span class="sd">        &#39;.gz&#39;, &#39;.bz2&#39;, &#39;.xz&#39; or &#39;.lzma&#39;) will be used automatically.</span>

<span class="sd">    * `store_objective` [boolean, default=True]:</span>
<span class="sd">        Whether the objective function should be stored. Set `store_objective`</span>
<span class="sd">        to `False` if your objective function (`.specs[&#39;args&#39;][&#39;func&#39;]`) is</span>
<span class="sd">        unserializable (i.e. if an exception is raised when trying to serialize</span>
<span class="sd">        the optimization result).</span>

<span class="sd">        Notice that if `store_objective` is set to `False`, a deep copy of the</span>
<span class="sd">        optimization result is created, potentially leading to performance</span>
<span class="sd">        problems if `res` is very large. If the objective function is not</span>
<span class="sd">        critical, one can delete it before calling `skopt.dump()` and thus</span>
<span class="sd">        avoid deep copying of `res`.</span>

<span class="sd">    * `**kwargs` [other keyword arguments]:</span>
<span class="sd">        All other keyword arguments will be passed to `joblib.dump`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">store_objective</span><span class="p">:</span>
        <span class="n">dump_</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="s1">&#39;func&#39;</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">specs</span><span class="p">[</span><span class="s1">&#39;args&#39;</span><span class="p">]:</span>
        <span class="c1"># If the user does not want to store the objective and it is indeed</span>
        <span class="c1"># present in the provided object, then create a deep copy of it and</span>
        <span class="c1"># remove the objective function before dumping it with joblib.dump.</span>
        <span class="n">res_without_func</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">res_without_func</span><span class="o">.</span><span class="n">specs</span><span class="p">[</span><span class="s1">&#39;args&#39;</span><span class="p">][</span><span class="s1">&#39;func&#39;</span><span class="p">]</span>
        <span class="n">dump_</span><span class="p">(</span><span class="n">res_without_func</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If the user does not want to store the objective and it is already</span>
        <span class="c1"># missing in the provided object, dump it without copying.</span>
        <span class="n">dump_</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.expected_minimum">
    <p>def <span class="ident">expected_minimum</span>(</p><p>res, n_random_starts=20, random_state=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Compute the minimum over the predictions of the last surrogate model.</p>
<p>Note that the returned minimum may not necessarily be an accurate
prediction of the minimum of the true objective function.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>res</code>  [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned by a <code>skopt</code> minimizer.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=20]:
    The number of random starts for the minimization of the surrogate
    model.</p>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>x</code> [list]: location of the minimum.</p>
</li>
<li>
<p><code>fun</code> [float]: the surrogate function value at the minimum.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.expected_minimum', this);">Show source &equiv;</a></p>
  <div id="source-skopt.expected_minimum" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">expected_minimum</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the minimum over the predictions of the last surrogate model.</span>

<span class="sd">    Note that the returned minimum may not necessarily be an accurate</span>
<span class="sd">    prediction of the minimum of the true objective function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `res`  [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned by a `skopt` minimizer.</span>

<span class="sd">    * `n_random_starts` [int, default=20]:</span>
<span class="sd">        The number of random starts for the minimization of the surrogate</span>
<span class="sd">        model.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `x` [list]: location of the minimum.</span>

<span class="sd">    * `fun` [float]: the surrogate function value at the minimum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">reg</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">n_random_starts</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_random_starts</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">))</span>

    <span class="n">best_x</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_fun</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">sp_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">res</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">fun</span> <span class="o">&lt;</span> <span class="n">best_fun</span><span class="p">:</span>
            <span class="n">best_x</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">x</span>
            <span class="n">best_fun</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">fun</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">best_x</span><span class="p">],</span> <span class="n">best_fun</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.forest_minimize">
    <p>def <span class="ident">forest_minimize</span>(</p><p>func, dimensions, base_estimator=&#39;ET&#39;, n_calls=100, n_random_starts=10, acq_func=&#39;EI&#39;, x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=10000, xi=0.01, kappa=1.96, n_jobs=1)</p>
    </div>
    

    
  
    <div class="desc"><p>Sequential optimisation using decision trees.</p>
<p>A tree based regression model is used to model the expensive to evaluate
function <code>func</code>. The model is improved by sequentially evaluating
the expensive function at the next best point. Thereby finding the
minimum of <code>func</code> with as few evaluations as possible.</p>
<p>The total number of evaluations, <code>n_calls</code>, are performed like the
following. If <code>x0</code> is provided but not <code>y0</code>, then the elements of <code>x0</code>
are first evaluated, followed by <code>n_random_starts</code> evaluations.
Finally, <code>n_calls - len(x0) - n_random_starts</code> evaluations are
made guided by the surrogate model. If <code>x0</code> and <code>y0</code> are both
provided then <code>n_random_starts</code> evaluations are first made then
<code>n_calls - n_random_starts</code> subsequent evaluations are made
guided by the surrogate model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a single list of parameters
    and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <code>skopt.utils.use_named_args</code> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <code>use_named_args</code> for an example.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(lower_bound, upper_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(lower_bound, upper_bound, prior)</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
<p>NOTE: The upper and lower bounds are inclusive for <code>Integer</code>
 dimensions.</p>
</li>
<li>
<p><code>base_estimator</code> [string or <code>Regressor</code>, default=<code>"ET"</code>]:
    The regressor to use as surrogate model. Can be either</p>
<ul>
<li><code>"RF"</code> for random forest regressor</li>
<li><code>"ET"</code> for extra trees regressor</li>
<li>instance of regressor with support for <code>return_std</code> in its predict
  method</li>
</ul>
<p>The predefined models are initilized with good defaults. If you
want to adjust the model parameters pass your own instance of
a regressor which returns the mean and standard deviation when
making predictions.</p>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code>.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    Number of evaluations of <code>func</code> with random points before
    approximating it with <code>base_estimator</code>.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"LCB"</code>]:
    Function to minimize over the forest posterior. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
<li>`"EIps" for negated expected improvement per second to take into
  account the function compute time. Then, the objective function is
  assumed to return two values, the first being the objective value and
  the second being the time taken in seconds.</li>
<li><code>"PIps"</code> for negated probability of improvement per second. The
  return type of the objective function is assumed to be similar to
  that of <code>"EIps"</code></li>
</ul>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]:
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, optional]
    If provided, then <code>callback(res)</code> is called after call to func.</p>
</li>
<li>
<p><code>n_points</code> [int, default=10000]:
    Number of points to sample when minimizing the acquisition function.</p>
</li>
<li>
<p><code>xi</code> [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Used when the acquisition is either <code>"EI"</code> or <code>"PI"</code>.</p>
</li>
<li>
<p><code>kappa</code> [float, default=1.96]:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    Used when the acquisition is <code>"LCB"</code>.</p>
</li>
<li>
<p><code>n_jobs</code> [int, default=1]:
    The number of jobs to run in parallel for <code>fit</code> and <code>predict</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>models</code>: surrogate models used for each iteration.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimization space.</li>
<li><code>specs</code> [dict]`: the call specifications.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.forest_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.forest_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forest_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="s2">&quot;ET&quot;</span><span class="p">,</span> <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span>
                    <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span>
                    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sequential optimisation using decision trees.</span>

<span class="sd">    A tree based regression model is used to model the expensive to evaluate</span>
<span class="sd">    function `func`. The model is improved by sequentially evaluating</span>
<span class="sd">    the expensive function at the next best point. Thereby finding the</span>
<span class="sd">    minimum of `func` with as few evaluations as possible.</span>

<span class="sd">    The total number of evaluations, `n_calls`, are performed like the</span>
<span class="sd">    following. If `x0` is provided but not `y0`, then the elements of `x0`</span>
<span class="sd">    are first evaluated, followed by `n_random_starts` evaluations.</span>
<span class="sd">    Finally, `n_calls - len(x0) - n_random_starts` evaluations are</span>
<span class="sd">    made guided by the surrogate model. If `x0` and `y0` are both</span>
<span class="sd">    provided then `n_random_starts` evaluations are first made then</span>
<span class="sd">    `n_calls - n_random_starts` subsequent evaluations are made</span>
<span class="sd">    guided by the surrogate model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a single list of parameters</span>
<span class="sd">        and return the objective value.</span>
<span class="sd">    </span>
<span class="sd">        If you have a search-space where all dimensions have names,</span>
<span class="sd">        then you can use `skopt.utils.use_named_args` as a decorator</span>
<span class="sd">        on your objective function, in order to call it directly</span>
<span class="sd">        with the named arguments. See `use_named_args` for an example.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(lower_bound, upper_bound, prior)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">         NOTE: The upper and lower bounds are inclusive for `Integer`</span>
<span class="sd">         dimensions.</span>

<span class="sd">    * `base_estimator` [string or `Regressor`, default=`&quot;ET&quot;`]:</span>
<span class="sd">        The regressor to use as surrogate model. Can be either</span>

<span class="sd">        - `&quot;RF&quot;` for random forest regressor</span>
<span class="sd">        - `&quot;ET&quot;` for extra trees regressor</span>
<span class="sd">        - instance of regressor with support for `return_std` in its predict</span>
<span class="sd">          method</span>

<span class="sd">        The predefined models are initilized with good defaults. If you</span>
<span class="sd">        want to adjust the model parameters pass your own instance of</span>
<span class="sd">        a regressor which returns the mean and standard deviation when</span>
<span class="sd">        making predictions.</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func`.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with random points before</span>
<span class="sd">        approximating it with `base_estimator`.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;LCB&quot;`]:</span>
<span class="sd">        Function to minimize over the forest posterior. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>
<span class="sd">        - `&quot;EIps&quot; for negated expected improvement per second to take into</span>
<span class="sd">          account the function compute time. Then, the objective function is</span>
<span class="sd">          assumed to return two values, the first being the objective value and</span>
<span class="sd">          the second being the time taken in seconds.</span>
<span class="sd">        - `&quot;PIps&quot;` for negated probability of improvement per second. The</span>
<span class="sd">          return type of the objective function is assumed to be similar to</span>
<span class="sd">          that of `&quot;EIps&quot;`</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]:</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, optional]</span>
<span class="sd">        If provided, then `callback(res)` is called after call to func.</span>

<span class="sd">    * `n_points` [int, default=10000]:</span>
<span class="sd">        Number of points to sample when minimizing the acquisition function.</span>

<span class="sd">    * `xi` [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Used when the acquisition is either `&quot;EI&quot;` or `&quot;PI&quot;`.</span>

<span class="sd">    * `kappa` [float, default=1.96]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        Used when the acquisition is `&quot;LCB&quot;`.</span>

<span class="sd">    * `n_jobs` [int, default=1]:</span>
<span class="sd">        The number of jobs to run in parallel for `fit` and `predict`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `models`: surrogate models used for each iteration.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimization space.</span>
<span class="sd">        - `specs` [dict]`: the call specifications.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="p">,</span>
                         <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">,</span>
                         <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_starts</span><span class="p">,</span>
                         <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                         <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
                         <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="n">kappa</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                         <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;sampling&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.gbrt_minimize">
    <p>def <span class="ident">gbrt_minimize</span>(</p><p>func, dimensions, base_estimator=None, n_calls=100, n_random_starts=10, acq_func=&#39;EI&#39;, acq_optimizer=&#39;auto&#39;, x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=10000, xi=0.01, kappa=1.96, n_jobs=1)</p>
    </div>
    

    
  
    <div class="desc"><p>Sequential optimization using gradient boosted trees.</p>
<p>Gradient boosted regression trees are used to model the (very)
expensive to evaluate function <code>func</code>. The model is improved
by sequentially evaluating the expensive function at the next
best point. Thereby finding the minimum of <code>func</code> with as
few evaluations as possible.</p>
<p>The total number of evaluations, <code>n_calls</code>, are performed like the
following. If <code>x0</code> is provided but not <code>y0</code>, then the elements of <code>x0</code>
are first evaluated, followed by <code>n_random_starts</code> evaluations.
Finally, <code>n_calls - len(x0) - n_random_starts</code> evaluations are
made guided by the surrogate model. If <code>x0</code> and <code>y0</code> are both
provided then <code>n_random_starts</code> evaluations are first made then
<code>n_calls - n_random_starts</code> subsequent evaluations are made
guided by the surrogate model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a single list of parameters
    and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <code>skopt.utils.use_named_args</code> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <code>use_named_args</code> for an example.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(lower_bound, upper_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(lower_bound, upper_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>base_estimator</code> [<code>GradientBoostingQuantileRegressor</code>]:
    The regressor to use as surrogate model</p>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code>.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    Number of evaluations of <code>func</code> with random points before
    approximating it with <code>base_estimator</code>.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"LCB"</code>]:
    Function to minimize over the forest posterior. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
<li><code>"EIps"</code> for negated expected improvement per second to take into
  account the function compute time. Then, the objective function is
  assumed to return two values, the first being the objective value and
  the second being the time taken.</li>
<li><code>"PIps"</code> for negated probability of improvement per second.</li>
</ul>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]:
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, optional]
    If provided, then <code>callback(res)</code> is called after call to func.</p>
</li>
<li>
<p><code>n_points</code> [int, default=10000]:
    Number of points to sample when minimizing the acquisition function.</p>
</li>
<li>
<p><code>xi</code> [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Used when the acquisition is either <code>"EI"</code> or <code>"PI"</code>.</p>
</li>
<li>
<p><code>kappa</code> [float, default=1.96]:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    Used when the acquisition is <code>"LCB"</code>.</p>
</li>
<li>
<p><code>n_jobs</code> [int, default=1]:
    The number of jobs to run in parallel for <code>fit</code> and <code>predict</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>models</code>: surrogate models used for each iteration.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimization space.</li>
<li><code>specs</code> [dict]`: the call specifications.</li>
<li><code>rng</code> [RandomState instance]: State of the random state
   at the end of minimization.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.gbrt_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.gbrt_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gbrt_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                  <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                  <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                  <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span>
                  <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sequential optimization using gradient boosted trees.</span>

<span class="sd">    Gradient boosted regression trees are used to model the (very)</span>
<span class="sd">    expensive to evaluate function `func`. The model is improved</span>
<span class="sd">    by sequentially evaluating the expensive function at the next</span>
<span class="sd">    best point. Thereby finding the minimum of `func` with as</span>
<span class="sd">    few evaluations as possible.</span>

<span class="sd">    The total number of evaluations, `n_calls`, are performed like the</span>
<span class="sd">    following. If `x0` is provided but not `y0`, then the elements of `x0`</span>
<span class="sd">    are first evaluated, followed by `n_random_starts` evaluations.</span>
<span class="sd">    Finally, `n_calls - len(x0) - n_random_starts` evaluations are</span>
<span class="sd">    made guided by the surrogate model. If `x0` and `y0` are both</span>
<span class="sd">    provided then `n_random_starts` evaluations are first made then</span>
<span class="sd">    `n_calls - n_random_starts` subsequent evaluations are made</span>
<span class="sd">    guided by the surrogate model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a single list of parameters</span>
<span class="sd">        and return the objective value.</span>
<span class="sd">    </span>
<span class="sd">        If you have a search-space where all dimensions have names,</span>
<span class="sd">        then you can use `skopt.utils.use_named_args` as a decorator</span>
<span class="sd">        on your objective function, in order to call it directly</span>
<span class="sd">        with the named arguments. See `use_named_args` for an example.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(lower_bound, upper_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `base_estimator` [`GradientBoostingQuantileRegressor`]:</span>
<span class="sd">        The regressor to use as surrogate model</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func`.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with random points before</span>
<span class="sd">        approximating it with `base_estimator`.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;LCB&quot;`]:</span>
<span class="sd">        Function to minimize over the forest posterior. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>
<span class="sd">        - ``&quot;EIps&quot;`` for negated expected improvement per second to take into</span>
<span class="sd">          account the function compute time. Then, the objective function is</span>
<span class="sd">          assumed to return two values, the first being the objective value and</span>
<span class="sd">          the second being the time taken.</span>
<span class="sd">        - `&quot;PIps&quot;` for negated probability of improvement per second.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]:</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, optional]</span>
<span class="sd">        If provided, then `callback(res)` is called after call to func.</span>

<span class="sd">    * `n_points` [int, default=10000]:</span>
<span class="sd">        Number of points to sample when minimizing the acquisition function.</span>

<span class="sd">    * `xi` [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Used when the acquisition is either `&quot;EI&quot;` or `&quot;PI&quot;`.</span>

<span class="sd">    * `kappa` [float, default=1.96]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        Used when the acquisition is `&quot;LCB&quot;`.</span>

<span class="sd">    * `n_jobs` [int, default=1]:</span>
<span class="sd">        The number of jobs to run in parallel for `fit` and `predict`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `models`: surrogate models used for each iteration.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimization space.</span>
<span class="sd">        - `specs` [dict]`: the call specifications.</span>
<span class="sd">        - `rng` [RandomState instance]: State of the random state</span>
<span class="sd">           at the end of minimization.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check params</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">cook_estimator</span><span class="p">(</span><span class="s2">&quot;GBRT&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
                                        <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="p">,</span>
                         <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">,</span>
                         <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_starts</span><span class="p">,</span>
                         <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span>
                         <span class="n">kappa</span><span class="o">=</span><span class="n">kappa</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                         <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;sampling&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.gp_minimize">
    <p>def <span class="ident">gp_minimize</span>(</p><p>func, dimensions, base_estimator=None, n_calls=100, n_random_starts=10, acq_func=&#39;gp_hedge&#39;, acq_optimizer=&#39;auto&#39;, x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=10000, n_restarts_optimizer=5, xi=0.01, kappa=1.96, noise=&#39;gaussian&#39;, n_jobs=1)</p>
    </div>
    

    
  
    <div class="desc"><p>Bayesian optimization using Gaussian Processes.</p>
<p>If every function evaluation is expensive, for instance
when the parameters are the hyperparameters of a neural network
and the function evaluation is the mean cross-validation score across
ten folds, optimizing the hyperparameters by standard optimization
routines would take for ever!</p>
<p>The idea is to approximate the function using a Gaussian process.
In other words the function values are assumed to follow a multivariate
gaussian. The covariance of the function values are given by a
GP kernel between the parameters. Then a smart choice to choose the
next parameter to evaluate can be made by the acquisition function
over the Gaussian prior which is much quicker to evaluate.</p>
<p>The total number of evaluations, <code>n_calls</code>, are performed like the
following. If <code>x0</code> is provided but not <code>y0</code>, then the elements of <code>x0</code>
are first evaluated, followed by <code>n_random_starts</code> evaluations.
Finally, <code>n_calls - len(x0) - n_random_starts</code> evaluations are
made guided by the surrogate model. If <code>x0</code> and <code>y0</code> are both
provided then <code>n_random_starts</code> evaluations are first made then
<code>n_calls - n_random_starts</code> subsequent evaluations are made
guided by the surrogate model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a single list of parameters
    and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <code>skopt.utils.use_named_args</code> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <code>use_named_args</code> for an example.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(lower_bound, upper_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(lower_bound, upper_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
<p>NOTE: The upper and lower bounds are inclusive for <code>Integer</code>
 dimensions.</p>
</li>
<li>
<p><code>base_estimator</code> [a Gaussian process estimator]:
    The Gaussian process estimator to use for optimization.
    By default, a Matern kernel is used with the following
    hyperparameters tuned.</p>
<ul>
<li>All the length scales of the Matern kernel.</li>
<li>The covariance amplitude that each element is multiplied with.</li>
<li>Noise that is added to the matern kernel. The noise is assumed
  to be iid gaussian.</li>
</ul>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code>.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    Number of evaluations of <code>func</code> with random points before
    approximating it with <code>base_estimator</code>.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"gp_hedge"</code>]:
    Function to minimize over the gaussian prior. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
<li><code>"gp_hedge"</code> Probabilistically choose one of the above three
  acquisition functions at every iteration. The weightage
  given to these gains can be set by <code>\eta</code> through <code>acq_func_kwargs</code>.<ul>
<li>The gains <code>g_i</code> are initialized to zero.</li>
<li>At every iteration,<ul>
<li>Each acquisition function is optimised independently to
  propose an candidate point <code>X_i</code>.</li>
<li>Out of all these candidate points, the next point <code>X_best</code> is
  chosen by <code>softmax(\eta g_i)</code></li>
<li>After fitting the surrogate model with <code>(X_best, y_best)</code>,
  the gains are updated such that <code>g_i -= \mu(X_i)</code></li>
</ul>
</li>
</ul>
</li>
<li><code>"EIps"</code> for negated expected improvement per second to take into
  account the function compute time. Then, the objective function is
  assumed to return two values, the first being the objective value and
  the second being the time taken in seconds.</li>
<li><code>"PIps"</code> for negated probability of improvement per second. The
  return type of the objective function is assumed to be similar to
  that of `"EIps</li>
</ul>
</li>
<li>
<p><code>acq_optimizer</code> [string, <code>"sampling"</code> or <code>"lbfgs"</code>, default=<code>"lbfgs"</code>]:
    Method to minimize the acquistion function. The fit model
    is updated with the optimal value obtained by optimizing <code>acq_func</code>
    with <code>acq_optimizer</code>.</p>
<p>The <code>acq_func</code> is computed at <code>n_points</code> sampled randomly.</p>
<ul>
<li>If set to <code>"auto"</code>, then <code>acq_optimizer</code> is configured on the
  basis of the space searched over.
  If the space is Categorical then this is set to be "sampling"`.</li>
<li>If set to <code>"sampling"</code>, then the point among these <code>n_points</code>
  where the <code>acq_func</code> is minimum is the next candidate minimum.</li>
<li>If set to <code>"lbfgs"</code>, then<ul>
<li>The <code>n_restarts_optimizer</code> no. of points which the acquisition
    function is least are taken as start points.</li>
<li><code>"lbfgs"</code> is run for 20 iterations with these points as initial
    points to find local minima.</li>
<li>The optimal of these local minima is used to update the prior.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, list of callables, optional]
    If callable then <code>callback(res)</code> is called after each call to <code>func</code>.
    If list of callables, then each callable in the list is called.</p>
</li>
<li>
<p><code>n_points</code> [int, default=10000]:
    Number of points to sample to determine the next "best" point.
    Useless if acq_optimizer is set to <code>"lbfgs"</code>.</p>
</li>
<li>
<p><code>n_restarts_optimizer</code> [int, default=5]:
    The number of restarts of the optimizer when <code>acq_optimizer</code>
    is <code>"lbfgs"</code>.</p>
</li>
<li>
<p><code>kappa</code> [float, default=1.96]:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    Used when the acquisition is <code>"LCB"</code>.</p>
</li>
<li>
<p><code>xi</code> [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Used when the acquisition is either <code>"EI"</code> or <code>"PI"</code>.</p>
</li>
<li>
<p><code>noise</code> [float, default="gaussian"]:</p>
<ul>
<li>Use noise="gaussian" if the objective returns noisy observations.
  The noise of each observation is assumed to be iid with
  mean zero and a fixed variance.</li>
<li>If the variance is known before-hand, this can be set directly
  to the variance of the noise.</li>
<li>Set this to a value close to zero (1e-10) if the function is
  noise-free. Setting to zero might cause stability issues.</li>
</ul>
</li>
<li>
<p><code>n_jobs</code> [int, default=1]
    Number of cores to run in parallel while running the lbfgs
    optimizations over the acquisition function. Valid only
    when <code>acq_optimizer</code> is set to "lbfgs."
    Defaults to 1 core. If <code>n_jobs=-1</code>, then number of jobs is set
    to number of cores.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>models</code>: surrogate models used for each iteration.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimization space.</li>
<li><code>specs</code> [dict]`: the call specifications.</li>
<li><code>rng</code> [RandomState instance]: State of the random state
   at the end of minimization.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.gp_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.gp_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gp_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;gp_hedge&quot;</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">n_points</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span>
                <span class="n">noise</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bayesian optimization using Gaussian Processes.</span>

<span class="sd">    If every function evaluation is expensive, for instance</span>
<span class="sd">    when the parameters are the hyperparameters of a neural network</span>
<span class="sd">    and the function evaluation is the mean cross-validation score across</span>
<span class="sd">    ten folds, optimizing the hyperparameters by standard optimization</span>
<span class="sd">    routines would take for ever!</span>

<span class="sd">    The idea is to approximate the function using a Gaussian process.</span>
<span class="sd">    In other words the function values are assumed to follow a multivariate</span>
<span class="sd">    gaussian. The covariance of the function values are given by a</span>
<span class="sd">    GP kernel between the parameters. Then a smart choice to choose the</span>
<span class="sd">    next parameter to evaluate can be made by the acquisition function</span>
<span class="sd">    over the Gaussian prior which is much quicker to evaluate.</span>

<span class="sd">    The total number of evaluations, `n_calls`, are performed like the</span>
<span class="sd">    following. If `x0` is provided but not `y0`, then the elements of `x0`</span>
<span class="sd">    are first evaluated, followed by `n_random_starts` evaluations.</span>
<span class="sd">    Finally, `n_calls - len(x0) - n_random_starts` evaluations are</span>
<span class="sd">    made guided by the surrogate model. If `x0` and `y0` are both</span>
<span class="sd">    provided then `n_random_starts` evaluations are first made then</span>
<span class="sd">    `n_calls - n_random_starts` subsequent evaluations are made</span>
<span class="sd">    guided by the surrogate model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a single list of parameters</span>
<span class="sd">        and return the objective value.</span>
<span class="sd">    </span>
<span class="sd">        If you have a search-space where all dimensions have names,</span>
<span class="sd">        then you can use `skopt.utils.use_named_args` as a decorator</span>
<span class="sd">        on your objective function, in order to call it directly</span>
<span class="sd">        with the named arguments. See `use_named_args` for an example.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(lower_bound, upper_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">         NOTE: The upper and lower bounds are inclusive for `Integer`</span>
<span class="sd">         dimensions.</span>

<span class="sd">    * `base_estimator` [a Gaussian process estimator]:</span>
<span class="sd">        The Gaussian process estimator to use for optimization.</span>
<span class="sd">        By default, a Matern kernel is used with the following</span>
<span class="sd">        hyperparameters tuned.</span>
<span class="sd">        - All the length scales of the Matern kernel.</span>
<span class="sd">        - The covariance amplitude that each element is multiplied with.</span>
<span class="sd">        - Noise that is added to the matern kernel. The noise is assumed</span>
<span class="sd">          to be iid gaussian.</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func`.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with random points before</span>
<span class="sd">        approximating it with `base_estimator`.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;gp_hedge&quot;`]:</span>
<span class="sd">        Function to minimize over the gaussian prior. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>
<span class="sd">        - `&quot;gp_hedge&quot;` Probabilistically choose one of the above three</span>
<span class="sd">          acquisition functions at every iteration. The weightage</span>
<span class="sd">          given to these gains can be set by `\eta` through `acq_func_kwargs`.</span>
<span class="sd">            - The gains `g_i` are initialized to zero.</span>
<span class="sd">            - At every iteration,</span>
<span class="sd">                - Each acquisition function is optimised independently to</span>
<span class="sd">                  propose an candidate point `X_i`.</span>
<span class="sd">                - Out of all these candidate points, the next point `X_best` is</span>
<span class="sd">                  chosen by `softmax(\eta g_i)`</span>
<span class="sd">                - After fitting the surrogate model with `(X_best, y_best)`,</span>
<span class="sd">                  the gains are updated such that `g_i -= \mu(X_i)`</span>
<span class="sd">        - `&quot;EIps&quot;` for negated expected improvement per second to take into</span>
<span class="sd">          account the function compute time. Then, the objective function is</span>
<span class="sd">          assumed to return two values, the first being the objective value and</span>
<span class="sd">          the second being the time taken in seconds.</span>
<span class="sd">        - `&quot;PIps&quot;` for negated probability of improvement per second. The</span>
<span class="sd">          return type of the objective function is assumed to be similar to</span>
<span class="sd">          that of `&quot;EIps</span>

<span class="sd">    * `acq_optimizer` [string, `&quot;sampling&quot;` or `&quot;lbfgs&quot;`, default=`&quot;lbfgs&quot;`]:</span>
<span class="sd">        Method to minimize the acquistion function. The fit model</span>
<span class="sd">        is updated with the optimal value obtained by optimizing `acq_func`</span>
<span class="sd">        with `acq_optimizer`.</span>

<span class="sd">        The `acq_func` is computed at `n_points` sampled randomly.</span>

<span class="sd">        - If set to `&quot;auto&quot;`, then `acq_optimizer` is configured on the</span>
<span class="sd">          basis of the space searched over.</span>
<span class="sd">          If the space is Categorical then this is set to be &quot;sampling&quot;`.</span>
<span class="sd">        - If set to `&quot;sampling&quot;`, then the point among these `n_points`</span>
<span class="sd">          where the `acq_func` is minimum is the next candidate minimum.</span>
<span class="sd">        - If set to `&quot;lbfgs&quot;`, then</span>
<span class="sd">              - The `n_restarts_optimizer` no. of points which the acquisition</span>
<span class="sd">                function is least are taken as start points.</span>
<span class="sd">              - `&quot;lbfgs&quot;` is run for 20 iterations with these points as initial</span>
<span class="sd">                points to find local minima.</span>
<span class="sd">              - The optimal of these local minima is used to update the prior.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, list of callables, optional]</span>
<span class="sd">        If callable then `callback(res)` is called after each call to `func`.</span>
<span class="sd">        If list of callables, then each callable in the list is called.</span>

<span class="sd">    * `n_points` [int, default=10000]:</span>
<span class="sd">        Number of points to sample to determine the next &quot;best&quot; point.</span>
<span class="sd">        Useless if acq_optimizer is set to `&quot;lbfgs&quot;`.</span>

<span class="sd">    * `n_restarts_optimizer` [int, default=5]:</span>
<span class="sd">        The number of restarts of the optimizer when `acq_optimizer`</span>
<span class="sd">        is `&quot;lbfgs&quot;`.</span>

<span class="sd">    * `kappa` [float, default=1.96]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        Used when the acquisition is `&quot;LCB&quot;`.</span>

<span class="sd">    * `xi` [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Used when the acquisition is either `&quot;EI&quot;` or `&quot;PI&quot;`.</span>

<span class="sd">    * `noise` [float, default=&quot;gaussian&quot;]:</span>
<span class="sd">        - Use noise=&quot;gaussian&quot; if the objective returns noisy observations.</span>
<span class="sd">          The noise of each observation is assumed to be iid with</span>
<span class="sd">          mean zero and a fixed variance.</span>
<span class="sd">        - If the variance is known before-hand, this can be set directly</span>
<span class="sd">          to the variance of the noise.</span>
<span class="sd">        - Set this to a value close to zero (1e-10) if the function is</span>
<span class="sd">          noise-free. Setting to zero might cause stability issues.</span>

<span class="sd">    * `n_jobs` [int, default=1]</span>
<span class="sd">        Number of cores to run in parallel while running the lbfgs</span>
<span class="sd">        optimizations over the acquisition function. Valid only</span>
<span class="sd">        when `acq_optimizer` is set to &quot;lbfgs.&quot;</span>
<span class="sd">        Defaults to 1 core. If `n_jobs=-1`, then number of jobs is set</span>
<span class="sd">        to number of cores.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `models`: surrogate models used for each iteration.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimization space.</span>
<span class="sd">        - `specs` [dict]`: the call specifications.</span>
<span class="sd">        - `rng` [RandomState instance]: State of the random state</span>
<span class="sd">           at the end of minimization.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check params</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">space</span> <span class="o">=</span> <span class="n">normalize_dimensions</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">cook_estimator</span><span class="p">(</span>
            <span class="s2">&quot;GP&quot;</span><span class="p">,</span> <span class="n">space</span><span class="o">=</span><span class="n">space</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span>
            <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">space</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
        <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
        <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="n">kappa</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="n">acq_optimizer</span><span class="p">,</span> <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span>
        <span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_starts</span><span class="p">,</span>
        <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="n">n_restarts_optimizer</span><span class="p">,</span>
        <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.load">
    <p>def <span class="ident">load</span>(</p><p>filename, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Reconstruct a skopt optimization result from a file
persisted with skopt.dump.</p>
<p>Notice that the loaded optimization result can be missing
the objective function (<code>.specs['args']['func']</code>) if <a href="#skopt.dump"><code>dump</code></a>
was called with <code>store_objective=False</code>.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>filename</code> [string or <code>pathlib.Path</code>]:
    The path of the file from which to load the optimization result.</p>
</li>
<li>
<p><code>**kwargs</code> [other keyword arguments]:
    All other keyword arguments will be passed to <code>joblib.load</code>.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    Reconstructed OptimizeResult instance.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.load', this);">Show source &equiv;</a></p>
  <div id="source-skopt.load" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstruct a skopt optimization result from a file</span>
<span class="sd">    persisted with skopt.dump.</span>

<span class="sd">    Notice that the loaded optimization result can be missing</span>
<span class="sd">    the objective function (`.specs[&#39;args&#39;][&#39;func&#39;]`) if `skopt.dump`</span>
<span class="sd">    was called with `store_objective=False`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `filename` [string or `pathlib.Path`]:</span>
<span class="sd">        The path of the file from which to load the optimization result.</span>

<span class="sd">    * `**kwargs` [other keyword arguments]:</span>
<span class="sd">        All other keyword arguments will be passed to `joblib.load`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        Reconstructed OptimizeResult instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">load_</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  

    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="skopt.BayesSearchCV" class="name">class <span class="ident">BayesSearchCV</span></p>
      
  
    <div class="desc"><p>Bayesian optimization over hyper parameters.</p>
<p>BayesSearchCV implements a "fit" and a "score" method.
It also implements "predict", "predict_proba", "decision_function",
"transform" and "inverse_transform" if they are implemented in the
estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated search over parameter settings.</p>
<p>In contrast to GridSearchCV, not all parameter values are tried out, but
rather a fixed number of parameter settings is sampled from the specified
distributions. The number of parameter settings that are tried is
given by n_iter.</p>
<p>Parameters are presented as a list of skopt.space.Dimension objects.</p>
<h2>Parameters</h2>
<p>estimator : estimator object.
    A object of that type is instantiated for each search point.
    This object is assumed to implement the scikit-learn estimator api.
    Either estimator needs to provide a <code>score</code> function,
    or <code>scoring</code> must be passed.</p>
<p>search_spaces : dict, list of dict or list of tuple containing
    (dict, int).
    One of these cases:
    1. dictionary, where keys are parameter names (strings)
    and values are skopt.space.Dimension instances (Real, Integer
    or Categorical) or any other valid value that defines skopt
    dimension (see skopt.Optimizer docs). Represents search space
    over parameters of the provided estimator.
    2. list of dictionaries: a list of dictionaries, where every
    dictionary fits the description given in case 1 above.
    If a list of dictionary objects is given, then the search is
    performed sequentially for every parameter space with maximum
    number of evaluations set to self.n_iter.
    3. list of (dict, int &gt; 0): an extension of case 2 above,
    where first element of every tuple is a dictionary representing
    some search subspace, similarly as in case 2, and second element
    is a number of iterations that will be spent optimizing over
    this subspace.</p>
<p>n_iter : int, default=50
    Number of parameter settings that are sampled. n_iter trades
    off runtime vs quality of the solution. Consider increasing
    <code>n_points</code> if you want to try more parameter settings in
    parallel.</p>
<p>optimizer_kwargs : dict, optional
    Dict of arguments passed to :class:<code>Optimizer</code>.  For example,
    <code>{'base_estimator': 'RF'}</code> would use a Random Forest surrogate
    instead of the default Gaussian Process.</p>
<p>scoring : string, callable or None, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    <code>scorer(estimator, X, y)</code>.
    If <code>None</code>, the <code>score</code> method of the estimator is used.</p>
<p>fit_params : dict, optional
    Parameters to pass to the fit method.</p>
<p>n_jobs : int, default=1
    Number of jobs to run in parallel. At maximum there are
    <code>n_points</code> times <code>cv</code> jobs available during each iteration.</p>
<p>n_points : int, default=1
    Number of parameter settings to sample in parallel. If this does
    not align with <code>n_iter</code>, the last iteration will sample less
    points. See also :func:<code>~Optimizer.ask</code></p>
<p>pre_dispatch : int, or string, optional
    Controls the number of jobs that get dispatched during parallel
    execution. Reducing this number can be useful to avoid an
    explosion of memory consumption when more jobs get dispatched
    than CPUs can process. This parameter can be:</p>
<div class="codehilite"><pre><span></span>    - None, in which case all the jobs are immediately
      created and spawned. Use this for lightweight and
      fast-running jobs, to avoid delays due to on-demand
      spawning of the jobs

    - An int, giving the exact number of total jobs that are
      spawned

    - A string, giving an expression as a function of n_jobs,
      as in &#39;2*n_jobs&#39;
</pre></div>


<p>iid : boolean, default=True
    If True, the data is assumed to be identically distributed across
    the folds, and the loss minimized is the total loss per sample,
    and not the mean loss across the folds.</p>
<p>cv : int, cross-validation generator or an iterable, optional
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:
      - None, to use the default 3-fold cross validation,
      - integer, to specify the number of folds in a <code>(Stratified)KFold</code>,
      - An object to be used as a cross-validation generator.
      - An iterable yielding train, test splits.</p>
<div class="codehilite"><pre><span></span>For integer/None inputs, if the estimator is a classifier and ``y`` is
either binary or multiclass, :class:`StratifiedKFold` is used. In all
other cases, :class:`KFold` is used.

Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
cross-validation strategies that can be used here.
</pre></div>


<p>refit : boolean, default=True
    Refit the best estimator with the entire dataset.
    If "False", it is impossible to make predictions using
    this RandomizedSearchCV instance after fitting.</p>
<p>verbose : integer
    Controls the verbosity: the higher, the more messages.</p>
<p>random_state : int or RandomState
    Pseudo random number generator state used for random uniform sampling
    from lists of possible values instead of scipy.stats distributions.</p>
<p>error_score : 'raise' (default) or numeric
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised. If a numeric value is given,
    FitFailedWarning is raised. This parameter does not affect the refit
    step, which will always raise the error.</p>
<p>return_train_score : boolean, default=False
    If <code>'True'</code>, the <code>cv_results_</code> attribute will include training
    scores.</p>
<h2>Example</h2>
<p>from skopt import BayesSearchCV</p>
<h1>parameter ranges are specified by one of below</h1>
<p>from skopt.space import Real, Categorical, Integer</p>
<p>from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split</p>
<p>X, y = load_iris(True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75,
                                                    random_state=0)</p>
<h1>log-uniform: understand as search over p = exp(x) by varying x</h1>
<p>opt = BayesSearchCV(
    SVC(),
    {
        'C': Real(1e-6, 1e+6, prior='log-uniform'),
        'gamma': Real(1e-6, 1e+1, prior='log-uniform'),
        'degree': Integer(1,8),
        'kernel': Categorical(['linear', 'poly', 'rbf']),
    },
    n_iter=32
)</p>
<h1>executes bayesian optimization</h1>
<p>opt.fit(X_train, y_train)</p>
<h1>model can be saved, used for predictions or scoring</h1>
<p>print(opt.score(X_test, y_test))</p>
<h2>Attributes</h2>
<p>cv_results_ : dict of numpy (masked) ndarrays
    A dict with keys as column headers and values as columns, that can be
    imported into a pandas <code>DataFrame</code>.</p>
<div class="codehilite"><pre><span></span>For instance the below given table

+--------------+-------------+-------------------+---+---------------+
| param_kernel | param_gamma | split0_test_score |...|rank_test_score|
+==============+=============+===================+===+===============+
|    &#39;rbf&#39;     |     0.1     |        0.8        |...|       2       |
+--------------+-------------+-------------------+---+---------------+
|    &#39;rbf&#39;     |     0.2     |        0.9        |...|       1       |
+--------------+-------------+-------------------+---+---------------+
|    &#39;rbf&#39;     |     0.3     |        0.7        |...|       1       |
+--------------+-------------+-------------------+---+---------------+

will be represented by a ``cv_results_`` dict of::

    {
    &#39;param_kernel&#39; : masked_array(data = [&#39;rbf&#39;, &#39;rbf&#39;, &#39;rbf&#39;],
                                  mask = False),
    &#39;param_gamma&#39;  : masked_array(data = [0.1 0.2 0.3], mask = False),
    &#39;split0_test_score&#39;  : [0.8, 0.9, 0.7],
    &#39;split1_test_score&#39;  : [0.82, 0.5, 0.7],
    &#39;mean_test_score&#39;    : [0.81, 0.7, 0.7],
    &#39;std_test_score&#39;     : [0.02, 0.2, 0.],
    &#39;rank_test_score&#39;    : [3, 1, 1],
    &#39;split0_train_score&#39; : [0.8, 0.9, 0.7],
    &#39;split1_train_score&#39; : [0.82, 0.5, 0.7],
    &#39;mean_train_score&#39;   : [0.81, 0.7, 0.7],
    &#39;std_train_score&#39;    : [0.03, 0.03, 0.04],
    &#39;mean_fit_time&#39;      : [0.73, 0.63, 0.43, 0.49],
    &#39;std_fit_time&#39;       : [0.01, 0.02, 0.01, 0.01],
    &#39;mean_score_time&#39;    : [0.007, 0.06, 0.04, 0.04],
    &#39;std_score_time&#39;     : [0.001, 0.002, 0.003, 0.005],
    &#39;params&#39; : [{&#39;kernel&#39; : &#39;rbf&#39;, &#39;gamma&#39; : 0.1}, ...],
    }

NOTE that the key ``&#39;params&#39;`` is used to store a list of parameter
settings dict for all the parameter candidates.

The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and
``std_score_time`` are all in seconds.
</pre></div>


<p>best_estimator_ : estimator
    Estimator that was chosen by the search, i.e. estimator
    which gave highest score (or smallest loss if specified)
    on the left out data. Not available if refit=False.</p>
<p>best_score_ : float
    Score of best_estimator on the left out data.</p>
<p>best_params_ : dict
    Parameter setting that gave the best results on the hold out data.</p>
<p>best_index_ : int
    The index (of the <code>cv_results_</code> arrays) which corresponds to the best
    candidate parameter setting.</p>
<div class="codehilite"><pre><span></span>The dict at ``search.cv_results_[&#39;params&#39;][search.best_index_]`` gives
the parameter setting for the best model, that gives the highest
mean score (``search.best_score_``).
</pre></div>


<p>scorer_ : function
    Scorer function used on the held out data to choose the best
    parameters for the model.</p>
<p>n_splits_ : int
    The number of cross-validation splits (folds/iterations).</p>
<h2>Notes</h2>
<p>The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.</p>
<p>If <code>n_jobs</code> was set to a value higher than one, the data is copied for each
parameter setting(and not <code>n_jobs</code> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <code>pre_dispatch</code>. Then, the memory is copied only
<code>pre_dispatch</code> many times. A reasonable value for <code>pre_dispatch</code> is <code>2 *
n_jobs</code>.</p>
<h2>See Also</h2>
<p>:class:<code>GridSearchCV</code>:
    Does exhaustive search over a grid of parameters.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.BayesSearchCV', this);">Show source &equiv;</a></p>
  <div id="source-skopt.BayesSearchCV" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">BayesSearchCV</span><span class="p">(</span><span class="n">BaseSearchCV</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bayesian optimization over hyper parameters.</span>

<span class="sd">    BayesSearchCV implements a &quot;fit&quot; and a &quot;score&quot; method.</span>
<span class="sd">    It also implements &quot;predict&quot;, &quot;predict_proba&quot;, &quot;decision_function&quot;,</span>
<span class="sd">    &quot;transform&quot; and &quot;inverse_transform&quot; if they are implemented in the</span>
<span class="sd">    estimator used.</span>

<span class="sd">    The parameters of the estimator used to apply these methods are optimized</span>
<span class="sd">    by cross-validated search over parameter settings.</span>

<span class="sd">    In contrast to GridSearchCV, not all parameter values are tried out, but</span>
<span class="sd">    rather a fixed number of parameter settings is sampled from the specified</span>
<span class="sd">    distributions. The number of parameter settings that are tried is</span>
<span class="sd">    given by n_iter.</span>

<span class="sd">    Parameters are presented as a list of skopt.space.Dimension objects.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator object.</span>
<span class="sd">        A object of that type is instantiated for each search point.</span>
<span class="sd">        This object is assumed to implement the scikit-learn estimator api.</span>
<span class="sd">        Either estimator needs to provide a ``score`` function,</span>
<span class="sd">        or ``scoring`` must be passed.</span>

<span class="sd">    search_spaces : dict, list of dict or list of tuple containing</span>
<span class="sd">        (dict, int).</span>
<span class="sd">        One of these cases:</span>
<span class="sd">        1. dictionary, where keys are parameter names (strings)</span>
<span class="sd">        and values are skopt.space.Dimension instances (Real, Integer</span>
<span class="sd">        or Categorical) or any other valid value that defines skopt</span>
<span class="sd">        dimension (see skopt.Optimizer docs). Represents search space</span>
<span class="sd">        over parameters of the provided estimator.</span>
<span class="sd">        2. list of dictionaries: a list of dictionaries, where every</span>
<span class="sd">        dictionary fits the description given in case 1 above.</span>
<span class="sd">        If a list of dictionary objects is given, then the search is</span>
<span class="sd">        performed sequentially for every parameter space with maximum</span>
<span class="sd">        number of evaluations set to self.n_iter.</span>
<span class="sd">        3. list of (dict, int &gt; 0): an extension of case 2 above,</span>
<span class="sd">        where first element of every tuple is a dictionary representing</span>
<span class="sd">        some search subspace, similarly as in case 2, and second element</span>
<span class="sd">        is a number of iterations that will be spent optimizing over</span>
<span class="sd">        this subspace.</span>

<span class="sd">    n_iter : int, default=50</span>
<span class="sd">        Number of parameter settings that are sampled. n_iter trades</span>
<span class="sd">        off runtime vs quality of the solution. Consider increasing</span>
<span class="sd">        ``n_points`` if you want to try more parameter settings in</span>
<span class="sd">        parallel.</span>

<span class="sd">    optimizer_kwargs : dict, optional</span>
<span class="sd">        Dict of arguments passed to :class:`Optimizer`.  For example,</span>
<span class="sd">        ``{&#39;base_estimator&#39;: &#39;RF&#39;}`` would use a Random Forest surrogate</span>
<span class="sd">        instead of the default Gaussian Process.</span>

<span class="sd">    scoring : string, callable or None, default=None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``.</span>
<span class="sd">        If ``None``, the ``score`` method of the estimator is used.</span>

<span class="sd">    fit_params : dict, optional</span>
<span class="sd">        Parameters to pass to the fit method.</span>

<span class="sd">    n_jobs : int, default=1</span>
<span class="sd">        Number of jobs to run in parallel. At maximum there are</span>
<span class="sd">        ``n_points`` times ``cv`` jobs available during each iteration.</span>

<span class="sd">    n_points : int, default=1</span>
<span class="sd">        Number of parameter settings to sample in parallel. If this does</span>
<span class="sd">        not align with ``n_iter``, the last iteration will sample less</span>
<span class="sd">        points. See also :func:`~Optimizer.ask`</span>

<span class="sd">    pre_dispatch : int, or string, optional</span>
<span class="sd">        Controls the number of jobs that get dispatched during parallel</span>
<span class="sd">        execution. Reducing this number can be useful to avoid an</span>
<span class="sd">        explosion of memory consumption when more jobs get dispatched</span>
<span class="sd">        than CPUs can process. This parameter can be:</span>

<span class="sd">            - None, in which case all the jobs are immediately</span>
<span class="sd">              created and spawned. Use this for lightweight and</span>
<span class="sd">              fast-running jobs, to avoid delays due to on-demand</span>
<span class="sd">              spawning of the jobs</span>

<span class="sd">            - An int, giving the exact number of total jobs that are</span>
<span class="sd">              spawned</span>

<span class="sd">            - A string, giving an expression as a function of n_jobs,</span>
<span class="sd">              as in &#39;2*n_jobs&#39;</span>

<span class="sd">    iid : boolean, default=True</span>
<span class="sd">        If True, the data is assumed to be identically distributed across</span>
<span class="sd">        the folds, and the loss minimized is the total loss per sample,</span>
<span class="sd">        and not the mean loss across the folds.</span>

<span class="sd">    cv : int, cross-validation generator or an iterable, optional</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>
<span class="sd">        Possible inputs for cv are:</span>
<span class="sd">          - None, to use the default 3-fold cross validation,</span>
<span class="sd">          - integer, to specify the number of folds in a `(Stratified)KFold`,</span>
<span class="sd">          - An object to be used as a cross-validation generator.</span>
<span class="sd">          - An iterable yielding train, test splits.</span>

<span class="sd">        For integer/None inputs, if the estimator is a classifier and ``y`` is</span>
<span class="sd">        either binary or multiclass, :class:`StratifiedKFold` is used. In all</span>
<span class="sd">        other cases, :class:`KFold` is used.</span>

<span class="sd">        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various</span>
<span class="sd">        cross-validation strategies that can be used here.</span>

<span class="sd">    refit : boolean, default=True</span>
<span class="sd">        Refit the best estimator with the entire dataset.</span>
<span class="sd">        If &quot;False&quot;, it is impossible to make predictions using</span>
<span class="sd">        this RandomizedSearchCV instance after fitting.</span>

<span class="sd">    verbose : integer</span>
<span class="sd">        Controls the verbosity: the higher, the more messages.</span>

<span class="sd">    random_state : int or RandomState</span>
<span class="sd">        Pseudo random number generator state used for random uniform sampling</span>
<span class="sd">        from lists of possible values instead of scipy.stats distributions.</span>

<span class="sd">    error_score : &#39;raise&#39; (default) or numeric</span>
<span class="sd">        Value to assign to the score if an error occurs in estimator fitting.</span>
<span class="sd">        If set to &#39;raise&#39;, the error is raised. If a numeric value is given,</span>
<span class="sd">        FitFailedWarning is raised. This parameter does not affect the refit</span>
<span class="sd">        step, which will always raise the error.</span>

<span class="sd">    return_train_score : boolean, default=False</span>
<span class="sd">        If ``&#39;True&#39;``, the ``cv_results_`` attribute will include training</span>
<span class="sd">        scores.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>

<span class="sd">    from skopt import BayesSearchCV</span>
<span class="sd">    # parameter ranges are specified by one of below</span>
<span class="sd">    from skopt.space import Real, Categorical, Integer</span>

<span class="sd">    from sklearn.datasets import load_iris</span>
<span class="sd">    from sklearn.svm import SVC</span>
<span class="sd">    from sklearn.model_selection import train_test_split</span>

<span class="sd">    X, y = load_iris(True)</span>
<span class="sd">    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75,</span>
<span class="sd">                                                        random_state=0)</span>

<span class="sd">    # log-uniform: understand as search over p = exp(x) by varying x</span>
<span class="sd">    opt = BayesSearchCV(</span>
<span class="sd">        SVC(),</span>
<span class="sd">        {</span>
<span class="sd">            &#39;C&#39;: Real(1e-6, 1e+6, prior=&#39;log-uniform&#39;),</span>
<span class="sd">            &#39;gamma&#39;: Real(1e-6, 1e+1, prior=&#39;log-uniform&#39;),</span>
<span class="sd">            &#39;degree&#39;: Integer(1,8),</span>
<span class="sd">            &#39;kernel&#39;: Categorical([&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;]),</span>
<span class="sd">        },</span>
<span class="sd">        n_iter=32</span>
<span class="sd">    )</span>

<span class="sd">    # executes bayesian optimization</span>
<span class="sd">    opt.fit(X_train, y_train)</span>

<span class="sd">    # model can be saved, used for predictions or scoring</span>
<span class="sd">    print(opt.score(X_test, y_test))</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    cv_results_ : dict of numpy (masked) ndarrays</span>
<span class="sd">        A dict with keys as column headers and values as columns, that can be</span>
<span class="sd">        imported into a pandas ``DataFrame``.</span>

<span class="sd">        For instance the below given table</span>

<span class="sd">        +--------------+-------------+-------------------+---+---------------+</span>
<span class="sd">        | param_kernel | param_gamma | split0_test_score |...|rank_test_score|</span>
<span class="sd">        +==============+=============+===================+===+===============+</span>
<span class="sd">        |    &#39;rbf&#39;     |     0.1     |        0.8        |...|       2       |</span>
<span class="sd">        +--------------+-------------+-------------------+---+---------------+</span>
<span class="sd">        |    &#39;rbf&#39;     |     0.2     |        0.9        |...|       1       |</span>
<span class="sd">        +--------------+-------------+-------------------+---+---------------+</span>
<span class="sd">        |    &#39;rbf&#39;     |     0.3     |        0.7        |...|       1       |</span>
<span class="sd">        +--------------+-------------+-------------------+---+---------------+</span>

<span class="sd">        will be represented by a ``cv_results_`` dict of::</span>

<span class="sd">            {</span>
<span class="sd">            &#39;param_kernel&#39; : masked_array(data = [&#39;rbf&#39;, &#39;rbf&#39;, &#39;rbf&#39;],</span>
<span class="sd">                                          mask = False),</span>
<span class="sd">            &#39;param_gamma&#39;  : masked_array(data = [0.1 0.2 0.3], mask = False),</span>
<span class="sd">            &#39;split0_test_score&#39;  : [0.8, 0.9, 0.7],</span>
<span class="sd">            &#39;split1_test_score&#39;  : [0.82, 0.5, 0.7],</span>
<span class="sd">            &#39;mean_test_score&#39;    : [0.81, 0.7, 0.7],</span>
<span class="sd">            &#39;std_test_score&#39;     : [0.02, 0.2, 0.],</span>
<span class="sd">            &#39;rank_test_score&#39;    : [3, 1, 1],</span>
<span class="sd">            &#39;split0_train_score&#39; : [0.8, 0.9, 0.7],</span>
<span class="sd">            &#39;split1_train_score&#39; : [0.82, 0.5, 0.7],</span>
<span class="sd">            &#39;mean_train_score&#39;   : [0.81, 0.7, 0.7],</span>
<span class="sd">            &#39;std_train_score&#39;    : [0.03, 0.03, 0.04],</span>
<span class="sd">            &#39;mean_fit_time&#39;      : [0.73, 0.63, 0.43, 0.49],</span>
<span class="sd">            &#39;std_fit_time&#39;       : [0.01, 0.02, 0.01, 0.01],</span>
<span class="sd">            &#39;mean_score_time&#39;    : [0.007, 0.06, 0.04, 0.04],</span>
<span class="sd">            &#39;std_score_time&#39;     : [0.001, 0.002, 0.003, 0.005],</span>
<span class="sd">            &#39;params&#39; : [{&#39;kernel&#39; : &#39;rbf&#39;, &#39;gamma&#39; : 0.1}, ...],</span>
<span class="sd">            }</span>

<span class="sd">        NOTE that the key ``&#39;params&#39;`` is used to store a list of parameter</span>
<span class="sd">        settings dict for all the parameter candidates.</span>

<span class="sd">        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and</span>
<span class="sd">        ``std_score_time`` are all in seconds.</span>

<span class="sd">    best_estimator_ : estimator</span>
<span class="sd">        Estimator that was chosen by the search, i.e. estimator</span>
<span class="sd">        which gave highest score (or smallest loss if specified)</span>
<span class="sd">        on the left out data. Not available if refit=False.</span>

<span class="sd">    best_score_ : float</span>
<span class="sd">        Score of best_estimator on the left out data.</span>

<span class="sd">    best_params_ : dict</span>
<span class="sd">        Parameter setting that gave the best results on the hold out data.</span>

<span class="sd">    best_index_ : int</span>
<span class="sd">        The index (of the ``cv_results_`` arrays) which corresponds to the best</span>
<span class="sd">        candidate parameter setting.</span>

<span class="sd">        The dict at ``search.cv_results_[&#39;params&#39;][search.best_index_]`` gives</span>
<span class="sd">        the parameter setting for the best model, that gives the highest</span>
<span class="sd">        mean score (``search.best_score_``).</span>

<span class="sd">    scorer_ : function</span>
<span class="sd">        Scorer function used on the held out data to choose the best</span>
<span class="sd">        parameters for the model.</span>

<span class="sd">    n_splits_ : int</span>
<span class="sd">        The number of cross-validation splits (folds/iterations).</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The parameters selected are those that maximize the score of the held-out</span>
<span class="sd">    data, according to the scoring parameter.</span>

<span class="sd">    If `n_jobs` was set to a value higher than one, the data is copied for each</span>
<span class="sd">    parameter setting(and not `n_jobs` times). This is done for efficiency</span>
<span class="sd">    reasons if individual jobs take very little time, but may raise errors if</span>
<span class="sd">    the dataset is large and not enough memory is available.  A workaround in</span>
<span class="sd">    this case is to set `pre_dispatch`. Then, the memory is copied only</span>
<span class="sd">    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *</span>
<span class="sd">    n_jobs`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :class:`GridSearchCV`:</span>
<span class="sd">        Does exhaustive search over a grid of parameters.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">search_spaces</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">fit_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_points</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pre_dispatch</span><span class="o">=</span><span class="s1">&#39;2*n_jobs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">error_score</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span> <span class="o">=</span> <span class="n">search_spaces</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_points</span> <span class="o">=</span> <span class="n">n_points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_search_space</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">BayesSearchCV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
             <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">fit_params</span><span class="o">=</span><span class="n">fit_params</span><span class="p">,</span>
             <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="n">iid</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="n">refit</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
             <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span> <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">,</span>
             <span class="n">return_train_score</span><span class="o">=</span><span class="n">return_train_score</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_search_space</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">search_space</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Checks whether the search space argument is correct&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">search_space</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The search_spaces parameter should contain at least one&quot;</span>
                <span class="s2">&quot;non-empty search space, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">search_space</span>
            <span class="p">)</span>

        <span class="c1"># check if space is a single dict, convert to list if so</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">search_space</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_space</span><span class="p">]</span>

        <span class="c1"># check if the structure of the space is proper</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># convert to just a list of dicts</span>
            <span class="n">dicts_only</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># 1. check the case when a tuple of space, n_iter is provided</span>
            <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">search_space</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;All tuples in list of search spaces should have&quot;</span>
                            <span class="s2">&quot;length 2, and contain (dict, int), got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">elem</span>
                        <span class="p">)</span>
                    <span class="n">subspace</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">elem</span>

                    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span> <span class="n">n_iter</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Number of iterations in search space should be&quot;</span>
                            <span class="s2">&quot;positive integer, got </span><span class="si">%s</span><span class="s2"> in tuple </span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span>
                            <span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">elem</span><span class="p">)</span>
                        <span class="p">)</span>

                    <span class="c1"># save subspaces here for further checking</span>
                    <span class="n">dicts_only</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subspace</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">dicts_only</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;A search space should be provided as a dict or&quot;</span>
                        <span class="s2">&quot;tuple (dict, int), got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">elem</span><span class="p">)</span>

            <span class="c1"># 2. check all the dicts for correctness of contents</span>
            <span class="k">for</span> <span class="n">subspace</span> <span class="ow">in</span> <span class="n">dicts_only</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">subspace</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">check_dimension</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Search space should be provided as a dict or list of dict,&quot;</span>
                <span class="s2">&quot;got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">search_space</span><span class="p">)</span>

    <span class="c1"># copied for compatibility with 0.19 sklearn from 0.18 BaseSearchCV</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">best_score_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;cv_results_&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span><span class="p">]</span>

    <span class="c1"># copied for compatibility with 0.19 sklearn from 0.18 BaseSearchCV</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">best_params_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;cv_results_&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span><span class="p">]</span>

    <span class="c1"># copied for compatibility with 0.19 sklearn from 0.18 BaseSearchCV</span>
    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">parameter_iterable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Actual fitting,  performing the search over parameters.</span>
<span class="sd">        Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X</span>
<span class="sd">                    .../sklearn/model_selection/_search.py</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">model_selection</span><span class="o">.</span><span class="n">_validation</span><span class="o">.</span><span class="n">check_cv</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="n">estimator</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span> <span class="o">=</span> <span class="n">check_scoring</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">indexable</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
        <span class="n">n_splits</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parameter_iterable</span><span class="p">,</span> <span class="n">Sized</span><span class="p">):</span>
            <span class="n">n_candidates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameter_iterable</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Fitting {0} folds for each of {1} candidates, totalling&quot;</span>
                  <span class="s2">&quot; {2} fits&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">,</span>
                                     <span class="n">n_candidates</span> <span class="o">*</span> <span class="n">n_splits</span><span class="p">))</span>

        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span>
        <span class="n">pre_dispatch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span>

        <span class="n">cv_iter</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span>
        <span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">model_selection</span><span class="o">.</span><span class="n">_validation</span><span class="o">.</span><span class="n">_fit_and_score</span><span class="p">)(</span>
                <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">),</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span><span class="p">,</span>
                <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span>
                <span class="n">fit_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span><span class="p">,</span>
                <span class="n">return_train_score</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">return_train_score</span><span class="p">,</span>
                <span class="n">return_n_test_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">return_times</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_parameters</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">error_score</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">error_score</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">parameter_iterable</span>
            <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv_iter</span><span class="p">)</span>

        <span class="c1"># if one choose to see train score, &quot;out&quot; will contain train score info</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_train_score</span><span class="p">:</span>
            <span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span><span class="p">,</span> <span class="n">test_sample_counts</span><span class="p">,</span>
             <span class="n">fit_time</span><span class="p">,</span> <span class="n">score_time</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">out</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="p">(</span><span class="n">test_scores</span><span class="p">,</span> <span class="n">test_sample_counts</span><span class="p">,</span>
             <span class="n">fit_time</span><span class="p">,</span> <span class="n">score_time</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">out</span><span class="p">)</span>

        <span class="n">candidate_params</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[::</span><span class="n">n_splits</span><span class="p">]</span>
        <span class="n">n_candidates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">)</span>

        <span class="n">results</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_store</span><span class="p">(</span><span class="n">key_name</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;A small helper to store the scores/times to the cv_results_&quot;&quot;&quot;</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_candidates</span><span class="p">,</span>
                                                              <span class="n">n_splits</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">splits</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">split_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
                    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;split</span><span class="si">%d</span><span class="s2">_</span><span class="si">%s</span><span class="s2">&quot;</span>
                            <span class="o">%</span> <span class="p">(</span><span class="n">split_i</span><span class="p">,</span> <span class="n">key_name</span><span class="p">)]</span> <span class="o">=</span> <span class="n">array</span><span class="p">[:,</span> <span class="n">split_i</span><span class="p">]</span>

            <span class="n">array_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="s1">&#39;mean_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">key_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">array_means</span>
            <span class="c1"># Weighted std is not directly available in numpy</span>
            <span class="n">array_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">array</span> <span class="o">-</span>
                                             <span class="n">array_means</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
                                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">))</span>
            <span class="n">results</span><span class="p">[</span><span class="s1">&#39;std_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">key_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">array_stds</span>

            <span class="k">if</span> <span class="n">rank</span><span class="p">:</span>
                <span class="n">results</span><span class="p">[</span><span class="s2">&quot;rank_</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">key_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
                    <span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">array_means</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># Computed the (weighted) mean and std for test scores alone</span>
        <span class="c1"># NOTE test_sample counts (weights) remain the same for all candidates</span>
        <span class="n">test_sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_sample_counts</span><span class="p">[:</span><span class="n">n_splits</span><span class="p">],</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

        <span class="n">_store</span><span class="p">(</span><span class="s1">&#39;test_score&#39;</span><span class="p">,</span> <span class="n">test_scores</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">weights</span><span class="o">=</span><span class="n">test_sample_counts</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iid</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_train_score</span><span class="p">:</span>
            <span class="n">_store</span><span class="p">(</span><span class="s1">&#39;train_score&#39;</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">_store</span><span class="p">(</span><span class="s1">&#39;fit_time&#39;</span><span class="p">,</span> <span class="n">fit_time</span><span class="p">)</span>
        <span class="n">_store</span><span class="p">(</span><span class="s1">&#39;score_time&#39;</span><span class="p">,</span> <span class="n">score_time</span><span class="p">)</span>

        <span class="n">best_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;rank_test_score&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">best_parameters</span> <span class="o">=</span> <span class="n">candidate_params</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>

        <span class="c1"># Use one MaskedArray and mask all the places where the param is not</span>
        <span class="c1"># applicable for that candidate. Use defaultdict as each candidate may</span>
        <span class="c1"># not contain all the params</span>
        <span class="n">param_results</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span>
                                            <span class="n">MaskedArray</span><span class="p">,</span>
                                            <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_candidates</span><span class="p">,),</span>
                                            <span class="n">mask</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                            <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">cand_i</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># An all masked empty array gets created for the key</span>
                <span class="c1"># `&quot;param_%s&quot; % name` at the first occurence of `name`.</span>
                <span class="c1"># Setting the value at an index also unmasks that index</span>
                <span class="n">param_results</span><span class="p">[</span><span class="s2">&quot;param_</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">][</span><span class="n">cand_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">param_results</span><span class="p">)</span>

        <span class="c1"># Store a list of param dicts at the key &#39;params&#39;</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate_params</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span> <span class="o">=</span> <span class="n">results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span> <span class="o">=</span> <span class="n">best_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_splits_</span> <span class="o">=</span> <span class="n">n_splits</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
            <span class="c1"># fit the best estimator using the entire dataset</span>
            <span class="c1"># clone first to work around broken estimators</span>
            <span class="n">best_estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
                <span class="o">**</span><span class="n">best_parameters</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">best_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">best_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span> <span class="o">=</span> <span class="n">best_estimator</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit_best_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the estimator copy with best parameters found to the</span>
<span class="sd">        provided data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Input data, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_output],</span>
<span class="sd">            Target relative to X for classification or regression.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_make_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params_space</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instantiate skopt Optimizer class.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        params_space : dict</span>
<span class="sd">            Represents parameter search space. The keys are parameter</span>
<span class="sd">            names (strings) and values are skopt.space.Dimension instances,</span>
<span class="sd">            one of Real, Integer or Categorical.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        optimizer: Instance of the `Optimizer` class used for for search</span>
<span class="sd">            in some parameter space.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;dimensions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dimensions_aslist</span><span class="p">(</span><span class="n">params_space</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimizer</span>

    <span class="k">def</span> <span class="nf">_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">search_space</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generate n_jobs parameters and evaluate them in parallel.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># get parameter values to evaluate</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">)</span>
        <span class="n">params_dict</span> <span class="o">=</span> <span class="p">[</span><span class="n">point_asdict</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>

        <span class="c1"># HACK: self.cv_results_ is reset at every call to _fit, keep current</span>
        <span class="n">all_cv_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span>

        <span class="c1"># HACK: this adds compatibility with different versions of sklearn</span>
        <span class="n">refit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">refit</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">params_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">refit</span> <span class="o">=</span> <span class="n">refit</span>

        <span class="c1"># merge existing and new cv_results_</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">:</span>
            <span class="n">all_cv_results</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span> <span class="o">=</span> <span class="n">all_cv_results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">])</span>

        <span class="c1"># feed the point and objective back into optimizer</span>
        <span class="n">local_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">][</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">):]</span>

        <span class="c1"># optimizer minimizes objective, hence provide negative score</span>
        <span class="k">return</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">score</span> <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">local_results</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">total_iterations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count total iterations that will be taken to explore</span>
<span class="sd">        all subspaces with `fit` method.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        max_iter: int, total number of iterations to explore</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">total_iter</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span><span class="p">:</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">space</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">elem</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">n_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span>

            <span class="n">total_iter</span> <span class="o">+=</span> <span class="n">n_iter</span>

        <span class="k">return</span> <span class="n">total_iter</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run fit on the estimator with randomly drawn parameters.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_output]</span>
<span class="sd">            Target relative to X for classification or regression (class</span>
<span class="sd">            labels should be integers or strings).</span>

<span class="sd">        groups : array-like, with shape (n_samples,), optional</span>
<span class="sd">            Group labels for the samples used while splitting the dataset into</span>
<span class="sd">            train/test set.</span>

<span class="sd">        callback: [callable, list of callables, optional]</span>
<span class="sd">            If callable then `callback(res)` is called after each parameter</span>
<span class="sd">            combination tested. If list of callables, then each callable in</span>
<span class="sd">            the list is called.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check if space is a single dict, convert to list if so</span>
        <span class="n">search_spaces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_spaces</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">search_spaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_spaces</span><span class="p">]</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="n">check_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span><span class="p">[</span><span class="s1">&#39;random_state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_state</span>

        <span class="c1"># Instantiate optimizers for all the search spaces.</span>
        <span class="n">optimizers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">search_space</span> <span class="ow">in</span> <span class="n">search_spaces</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">search_space</span> <span class="o">=</span> <span class="n">search_space</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">optimizers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_make_optimizer</span><span class="p">(</span><span class="n">search_space</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizers_</span> <span class="o">=</span> <span class="n">optimizers</span>  <span class="c1"># will save the states of the optimizers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="n">n_points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_points</span>

        <span class="k">for</span> <span class="n">search_space</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">search_spaces</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">):</span>
            <span class="c1"># if not provided with search subspace, n_iter is taken as</span>
            <span class="c1"># self.n_iter</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">search_space</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">search_space</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">n_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span>

            <span class="c1"># do the optimization for particular search space</span>
            <span class="k">while</span> <span class="n">n_iter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># when n_iter &lt; n_points points left for evaluation</span>
                <span class="n">n_points_adjusted</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">n_points</span><span class="p">)</span>

                <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">search_space</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                    <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="n">n_points_adjusted</span>
                <span class="p">)</span>
                <span class="n">n_iter</span> <span class="o">-=</span> <span class="n">n_points</span>

                <span class="k">if</span> <span class="n">eval_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">optim_result</span><span class="p">):</span>
                    <span class="k">break</span>

        <span class="c1"># Refit the best model on the the whole dataset</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_best_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.BayesSearchCV">BayesSearchCV</a></li>
          <li>sklearn.model_selection._search.BaseSearchCV</li>
          <li>abc.NewBase</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.MetaEstimatorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.BayesSearchCV.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, estimator, search_spaces, optimizer_kwargs=None, n_iter=50, scoring=None, fit_params=None, n_jobs=1, n_points=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=&#39;2*n_jobs&#39;, random_state=None, error_score=&#39;raise&#39;, return_train_score=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.BayesSearchCV.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.BayesSearchCV.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">search_spaces</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">fit_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">n_points</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">pre_dispatch</span><span class="o">=</span><span class="s1">&#39;2*n_jobs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">error_score</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span> <span class="o">=</span> <span class="n">search_spaces</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_points</span> <span class="o">=</span> <span class="n">n_points</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_search_space</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BayesSearchCV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
         <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">fit_params</span><span class="o">=</span><span class="n">fit_params</span><span class="p">,</span>
         <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="n">iid</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="n">refit</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
         <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span> <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">,</span>
         <span class="n">return_train_score</span><span class="o">=</span><span class="n">return_train_score</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.BayesSearchCV.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y=None, groups=None, callback=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Run fit on the estimator with randomly drawn parameters.</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix, shape = [n_samples, n_features]
    The training input samples.</p>
<p>y : array-like, shape = [n_samples] or [n_samples, n_output]
    Target relative to X for classification or regression (class
    labels should be integers or strings).</p>
<p>groups : array-like, with shape (n_samples,), optional
    Group labels for the samples used while splitting the dataset into
    train/test set.</p>
<p>callback: [callable, list of callables, optional]
    If callable then <code>callback(res)</code> is called after each parameter
    combination tested. If list of callables, then each callable in
    the list is called.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.BayesSearchCV.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.BayesSearchCV.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run fit on the estimator with randomly drawn parameters.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The training input samples.</span>
<span class="sd">    y : array-like, shape = [n_samples] or [n_samples, n_output]</span>
<span class="sd">        Target relative to X for classification or regression (class</span>
<span class="sd">        labels should be integers or strings).</span>
<span class="sd">    groups : array-like, with shape (n_samples,), optional</span>
<span class="sd">        Group labels for the samples used while splitting the dataset into</span>
<span class="sd">        train/test set.</span>
<span class="sd">    callback: [callable, list of callables, optional]</span>
<span class="sd">        If callable then `callback(res)` is called after each parameter</span>
<span class="sd">        combination tested. If list of callables, then each callable in</span>
<span class="sd">        the list is called.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># check if space is a single dict, convert to list if so</span>
    <span class="n">search_spaces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_spaces</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_spaces</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">search_spaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_spaces</span><span class="p">]</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">check_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs_</span><span class="p">[</span><span class="s1">&#39;random_state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_state</span>
    <span class="c1"># Instantiate optimizers for all the search spaces.</span>
    <span class="n">optimizers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">search_space</span> <span class="ow">in</span> <span class="n">search_spaces</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">search_space</span> <span class="o">=</span> <span class="n">search_space</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">optimizers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_make_optimizer</span><span class="p">(</span><span class="n">search_space</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizers_</span> <span class="o">=</span> <span class="n">optimizers</span>  <span class="c1"># will save the states of the optimizers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">n_points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_points</span>
    <span class="k">for</span> <span class="n">search_space</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">search_spaces</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">):</span>
        <span class="c1"># if not provided with search subspace, n_iter is taken as</span>
        <span class="c1"># self.n_iter</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">search_space</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">search_space</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">search_space</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span>
        <span class="c1"># do the optimization for particular search space</span>
        <span class="k">while</span> <span class="n">n_iter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># when n_iter &lt; n_points points left for evaluation</span>
            <span class="n">n_points_adjusted</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">n_points</span><span class="p">)</span>
            <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">search_space</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="n">n_points_adjusted</span>
            <span class="p">)</span>
            <span class="n">n_iter</span> <span class="o">-=</span> <span class="n">n_points</span>
            <span class="k">if</span> <span class="n">eval_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">optim_result</span><span class="p">):</span>
                <span class="k">break</span>
    <span class="c1"># Refit the best model on the the whole dataset</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fit_best_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.BayesSearchCV.best_params_" class="name">var <span class="ident">best_params_</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.best_score_" class="name">var <span class="ident">best_score_</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.classes_" class="name">var <span class="ident">classes_</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.grid_scores_" class="name">var <span class="ident">grid_scores_</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.n_iter" class="name">var <span class="ident">n_iter</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.n_points" class="name">var <span class="ident">n_points</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.optimizer_kwargs" class="name">var <span class="ident">optimizer_kwargs</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.random_state" class="name">var <span class="ident">random_state</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.search_spaces" class="name">var <span class="ident">search_spaces</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.BayesSearchCV.total_iterations" class="name">var <span class="ident">total_iterations</span></p>
            

            
  
    <div class="desc"><p>Count total iterations that will be taken to explore
all subspaces with <code>fit</code> method.</p>
<h2>Returns</h2>
<p>max_iter: int, total number of iterations to explore</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.Optimizer" class="name">class <span class="ident">Optimizer</span></p>
      
  
    <div class="desc"><p>Run bayesian optimisation loop.</p>
<p>An <code>Optimizer</code> represents the steps of a bayesian optimisation loop. To
use it you need to provide your own loop mechanism. The various
optimisers provided by <code>skopt</code> use this class under the hood.</p>
<p>Use this class directly if you want to control the iterations of your
bayesian optimisation loop.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(lower_bound, upper_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(lower_bound, upper_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>base_estimator</code> ["GP", "RF", "ET", "GBRT" or sklearn regressor, default="GP"]:
    Should inherit from <code>sklearn.base.RegressorMixin</code>.
    In addition the <code>predict</code> method, should have an optional <code>return_std</code>
    argument, which returns <code>std(Y | x)`` along with</code>E[Y | x]`.
    If base_estimator is one of ["GP", "RF", "ET", "GBRT"], a default
    surrogate model of the corresponding type is used corresponding to what
    is used in the minimize functions.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    DEPRECATED, use <code>n_initial_points</code> instead.</p>
</li>
<li>
<p><code>n_initial_points</code> [int, default=10]:
    Number of evaluations of <code>func</code> with initialization points
    before approximating it with <code>base_estimator</code>. Points provided as
    <code>x0</code> count as initialization points. If len(x0) &lt; n_initial_points
    additional points are sampled at random.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"gp_hedge"</code>]:
    Function to minimize over the posterior distribution. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
<li><code>"gp_hedge"</code> Probabilistically choose one of the above three
  acquisition functions at every iteration.<ul>
<li>The gains <code>g_i</code> are initialized to zero.</li>
<li>At every iteration,<ul>
<li>Each acquisition function is optimised independently to
  propose an candidate point <code>X_i</code>.</li>
<li>Out of all these candidate points, the next point <code>X_best</code> is
  chosen by $softmax(\eta g_i)$</li>
<li>After fitting the surrogate model with <code>(X_best, y_best)</code>,
  the gains are updated such that $g_i -= \mu(X_i)$</li>
</ul>
</li>
</ul>
</li>
<li>`"EIps" for negated expected improvement per second to take into
  account the function compute time. Then, the objective function is
  assumed to return two values, the first being the objective value and
  the second being the time taken in seconds.</li>
<li><code>"PIps"</code> for negated probability of improvement per second. The
  return type of the objective function is assumed to be similar to
  that of `"EIps</li>
</ul>
</li>
<li>
<p><code>acq_optimizer</code> [string, <code>"sampling"</code> or <code>"lbfgs"</code>, default=<code>"auto"</code>]:
    Method to minimize the acquistion function. The fit model
    is updated with the optimal value obtained by optimizing <code>acq_func</code>
    with <code>acq_optimizer</code>.</p>
<ul>
<li>If set to <code>"auto"</code>, then <code>acq_optimizer</code> is configured on the
  basis of the base_estimator and the space searched over.
  If the space is Categorical or if the estimator provided based on
  tree-models then this is set to be "sampling"`.</li>
<li>If set to <code>"sampling"</code>, then <code>acq_func</code> is optimized by computing
  <code>acq_func</code> at <code>n_points</code> randomly sampled points.</li>
<li>If set to <code>"lbfgs"</code>, then <code>acq_func</code> is optimized by<ul>
<li>Sampling <code>n_restarts_optimizer</code> points randomly.</li>
<li><code>"lbfgs"</code> is run for 20 iterations with these points as initial
    points to find local minima.</li>
<li>The optimal of these local minima is used to update the prior.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>acq_func_kwargs</code> [dict]:
    Additional arguments to be passed to the acquistion function.</p>
</li>
<li>
<p><code>acq_optimizer_kwargs</code> [dict]:
    Additional arguments to be passed to the acquistion optimizer.</p>
</li>
</ul>
<h2>Attributes</h2>
<ul>
<li><code>Xi</code> [list]:
    Points at which objective has been evaluated.</li>
<li><code>yi</code> [scalar]:
    Values of objective at corresponding points in <code>Xi</code>.</li>
<li><code>models</code> [list]:
    Regression models used to fit observations and compute acquisition
    function.</li>
<li><code>space</code>
    An instance of <code>skopt.space.Space</code>. Stores parameter search space used
    to sample points, bounds, and type of parameters.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Optimizer', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Optimizer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run bayesian optimisation loop.</span>

<span class="sd">    An `Optimizer` represents the steps of a bayesian optimisation loop. To</span>
<span class="sd">    use it you need to provide your own loop mechanism. The various</span>
<span class="sd">    optimisers provided by `skopt` use this class under the hood.</span>

<span class="sd">    Use this class directly if you want to control the iterations of your</span>
<span class="sd">    bayesian optimisation loop.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(lower_bound, upper_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `base_estimator` [&quot;GP&quot;, &quot;RF&quot;, &quot;ET&quot;, &quot;GBRT&quot; or sklearn regressor, default=&quot;GP&quot;]:</span>
<span class="sd">        Should inherit from `sklearn.base.RegressorMixin`.</span>
<span class="sd">        In addition the `predict` method, should have an optional `return_std`</span>
<span class="sd">        argument, which returns `std(Y | x)`` along with `E[Y | x]`.</span>
<span class="sd">        If base_estimator is one of [&quot;GP&quot;, &quot;RF&quot;, &quot;ET&quot;, &quot;GBRT&quot;], a default</span>
<span class="sd">        surrogate model of the corresponding type is used corresponding to what</span>
<span class="sd">        is used in the minimize functions.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        DEPRECATED, use `n_initial_points` instead.</span>

<span class="sd">    * `n_initial_points` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with initialization points</span>
<span class="sd">        before approximating it with `base_estimator`. Points provided as</span>
<span class="sd">        `x0` count as initialization points. If len(x0) &lt; n_initial_points</span>
<span class="sd">        additional points are sampled at random.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;gp_hedge&quot;`]:</span>
<span class="sd">        Function to minimize over the posterior distribution. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>
<span class="sd">        - `&quot;gp_hedge&quot;` Probabilistically choose one of the above three</span>
<span class="sd">          acquisition functions at every iteration.</span>
<span class="sd">            - The gains `g_i` are initialized to zero.</span>
<span class="sd">            - At every iteration,</span>
<span class="sd">                - Each acquisition function is optimised independently to</span>
<span class="sd">                  propose an candidate point `X_i`.</span>
<span class="sd">                - Out of all these candidate points, the next point `X_best` is</span>
<span class="sd">                  chosen by $softmax(\eta g_i)$</span>
<span class="sd">                - After fitting the surrogate model with `(X_best, y_best)`,</span>
<span class="sd">                  the gains are updated such that $g_i -= \mu(X_i)$</span>
<span class="sd">        - `&quot;EIps&quot; for negated expected improvement per second to take into</span>
<span class="sd">          account the function compute time. Then, the objective function is</span>
<span class="sd">          assumed to return two values, the first being the objective value and</span>
<span class="sd">          the second being the time taken in seconds.</span>
<span class="sd">        - `&quot;PIps&quot;` for negated probability of improvement per second. The</span>
<span class="sd">          return type of the objective function is assumed to be similar to</span>
<span class="sd">          that of `&quot;EIps</span>

<span class="sd">    * `acq_optimizer` [string, `&quot;sampling&quot;` or `&quot;lbfgs&quot;`, default=`&quot;auto&quot;`]:</span>
<span class="sd">        Method to minimize the acquistion function. The fit model</span>
<span class="sd">        is updated with the optimal value obtained by optimizing `acq_func`</span>
<span class="sd">        with `acq_optimizer`.</span>

<span class="sd">        - If set to `&quot;auto&quot;`, then `acq_optimizer` is configured on the</span>
<span class="sd">          basis of the base_estimator and the space searched over.</span>
<span class="sd">          If the space is Categorical or if the estimator provided based on</span>
<span class="sd">          tree-models then this is set to be &quot;sampling&quot;`.</span>
<span class="sd">        - If set to `&quot;sampling&quot;`, then `acq_func` is optimized by computing</span>
<span class="sd">          `acq_func` at `n_points` randomly sampled points.</span>
<span class="sd">        - If set to `&quot;lbfgs&quot;`, then `acq_func` is optimized by</span>
<span class="sd">              - Sampling `n_restarts_optimizer` points randomly.</span>
<span class="sd">              - `&quot;lbfgs&quot;` is run for 20 iterations with these points as initial</span>
<span class="sd">                points to find local minima.</span>
<span class="sd">              - The optimal of these local minima is used to update the prior.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `acq_func_kwargs` [dict]:</span>
<span class="sd">        Additional arguments to be passed to the acquistion function.</span>

<span class="sd">    * `acq_optimizer_kwargs` [dict]:</span>
<span class="sd">        Additional arguments to be passed to the acquistion optimizer.</span>


<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    * `Xi` [list]:</span>
<span class="sd">        Points at which objective has been evaluated.</span>
<span class="sd">    * `yi` [scalar]:</span>
<span class="sd">        Values of objective at corresponding points in `Xi`.</span>
<span class="sd">    * `models` [list]:</span>
<span class="sd">        Regression models used to fit observations and compute acquisition</span>
<span class="sd">        function.</span>
<span class="sd">    * `space`</span>
<span class="sd">        An instance of `skopt.space.Space`. Stores parameter search space used</span>
<span class="sd">        to sample points, bounds, and type of parameters.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="s2">&quot;gp&quot;</span><span class="p">,</span>
                 <span class="n">n_random_starts</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_initial_points</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;gp_hedge&quot;</span><span class="p">,</span>
                 <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">acq_optimizer_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Arguments that are just stored not checked</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="o">=</span> <span class="n">acq_func</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acq_func_kwargs</span> <span class="o">=</span> <span class="n">acq_func_kwargs</span>

        <span class="n">allowed_acq_funcs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gp_hedge&quot;</span><span class="p">,</span> <span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="s2">&quot;LCB&quot;</span><span class="p">,</span> <span class="s2">&quot;PI&quot;</span><span class="p">,</span> <span class="s2">&quot;EIps&quot;</span><span class="p">,</span> <span class="s2">&quot;PIps&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_acq_funcs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected acq_func to be in </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                             <span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">allowed_acq_funcs</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="o">==</span> <span class="s2">&quot;gp_hedge&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cand_acq_funcs_</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="s2">&quot;LCB&quot;</span><span class="p">,</span> <span class="s2">&quot;PI&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gains_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cand_acq_funcs_</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">acq_func_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">acq_func_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">acq_func_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eta&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">acq_optimizer_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">acq_optimizer_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_points</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_points&quot;</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_restarts_optimizer</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;n_restarts_optimizer&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">n_jobs</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer_kwargs</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span>

        <span class="k">if</span> <span class="n">n_random_starts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span><span class="s2">&quot;n_random_starts will be removed in favour of &quot;</span>
                           <span class="s2">&quot;n_initial_points.&quot;</span><span class="p">),</span>
                          <span class="ne">DeprecationWarning</span><span class="p">)</span>
            <span class="n">n_initial_points</span> <span class="o">=</span> <span class="n">n_random_starts</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_arguments</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">n_initial_points</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="p">,</span>
                              <span class="n">dimensions</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">,</span> <span class="n">GaussianProcessRegressor</span><span class="p">):</span>
            <span class="n">dimensions</span> <span class="o">=</span> <span class="n">normalize_dimensions</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">space</span> <span class="o">=</span> <span class="n">Space</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">yi</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_cat_inds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_non_cat_inds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensions</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cat_inds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_non_cat_inds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

        <span class="c1"># The cache of responses of `ask` method for n_points not None.</span>
        <span class="c1"># This ensures that multiple calls to `ask` with n_points set</span>
        <span class="c1"># return same sets of points.</span>
        <span class="c1"># The cache is reset to {} at every call to `tell`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">_check_arguments</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_estimator</span><span class="p">,</span> <span class="n">n_initial_points</span><span class="p">,</span>
                         <span class="n">acq_optimizer</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check arguments for sanity.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">cook_estimator</span><span class="p">(</span>
                <span class="n">base_estimator</span><span class="p">,</span> <span class="n">space</span><span class="o">=</span><span class="n">dimensions</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span> <span class="ow">and</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> has to be a regressor.&quot;</span> <span class="o">%</span> <span class="n">base_estimator</span><span class="p">)</span>

        <span class="n">is_multi_regressor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">MultiOutputRegressor</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_multi_regressor</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="o">=</span> <span class="n">MultiOutputRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="o">=</span> <span class="n">base_estimator</span>

        <span class="k">if</span> <span class="n">n_initial_points</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `n_initial_points` &gt;= 0, got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">n_initial_points</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">=</span> <span class="n">n_initial_points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_initial_points_</span> <span class="o">=</span> <span class="n">n_initial_points</span>

        <span class="k">if</span> <span class="n">acq_optimizer</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">has_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">):</span>
                <span class="n">acq_optimizer</span> <span class="o">=</span> <span class="s2">&quot;lbfgs&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">acq_optimizer</span> <span class="o">=</span> <span class="s2">&quot;sampling&quot;</span>

        <span class="k">if</span> <span class="n">acq_optimizer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">,</span> <span class="s2">&quot;sampling&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected acq_optimizer to be &#39;lbfgs&#39; or &quot;</span>
                             <span class="s2">&quot;&#39;sampling&#39;, got {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acq_optimizer</span><span class="p">))</span>

        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">has_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">)</span> <span class="ow">and</span>
            <span class="n">acq_optimizer</span> <span class="o">!=</span> <span class="s2">&quot;sampling&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The regressor {0} should run with &quot;</span>
                             <span class="s2">&quot;acq_optimizer&quot;</span>
                             <span class="s2">&quot;=&#39;sampling&#39;.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer</span> <span class="o">=</span> <span class="n">acq_optimizer</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create a shallow copy of an instance of the optimizer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">            Set the random state of the copy.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span>
            <span class="n">dimensions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensions</span><span class="p">,</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">,</span>
            <span class="n">n_initial_points</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_initial_points_</span><span class="p">,</span>
            <span class="n">acq_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">,</span>
            <span class="n">acq_optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer</span><span class="p">,</span>
            <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func_kwargs</span><span class="p">,</span>
            <span class="n">acq_optimizer_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer_kwargs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gains_&quot;</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">gains_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gains_</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimizer</span>

    <span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;cl_min&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Query point or multiple points at which objective should be evaluated.</span>

<span class="sd">        * `n_points` [int or None, default=None]:</span>
<span class="sd">            Number of points returned by the ask method.</span>
<span class="sd">            If the value is None, a single point to evaluate is returned.</span>
<span class="sd">            Otherwise a list of points to evaluate is returned of size</span>
<span class="sd">            n_points. This is useful if you can evaluate your objective in</span>
<span class="sd">            parallel, and thus obtain more objective function evaluations per</span>
<span class="sd">            unit of time.</span>

<span class="sd">        * `strategy` [string, default=`&quot;cl_min&quot;`]:</span>
<span class="sd">            Method to use to sample multiple points (see also `n_points`</span>
<span class="sd">            description). This parameter is ignored if n_points = None.</span>
<span class="sd">            Supported options are `&quot;cl_min&quot;`, `&quot;cl_mean&quot;` or `&quot;cl_max&quot;`.</span>

<span class="sd">            - If set to `&quot;cl_min&quot;`, then constant liar strtategy is used</span>
<span class="sd">               with lie objective value being minimum of observed objective</span>
<span class="sd">               values. `&quot;cl_mean&quot;` and `&quot;cl_max&quot;` means mean and max of values</span>
<span class="sd">               respectively. For details on this strategy see:</span>

<span class="sd">               https://hal.archives-ouvertes.fr/hal-00732512/document</span>

<span class="sd">               With this strategy a copy of optimizer is created, which is</span>
<span class="sd">               then asked for a point, and the point is told to the copy of</span>
<span class="sd">               optimizer with some fake objective (lie), the next point is</span>
<span class="sd">               asked from copy, it is also told to the copy with fake</span>
<span class="sd">               objective and so on. The type of lie defines different</span>
<span class="sd">               flavours of `cl_x` strategies.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">n_points</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ask</span><span class="p">()</span>

        <span class="n">supported_strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cl_min&quot;</span><span class="p">,</span> <span class="s2">&quot;cl_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;cl_max&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">n_points</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;n_points should be int &gt; 0, got &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_points</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_strategies</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected parallel_strategy to be one of &quot;</span> <span class="o">+</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">supported_strategies</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span> <span class="o">+</span> <span class="s2">&quot;got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">strategy</span>
            <span class="p">)</span>

        <span class="c1"># Caching the result with n_points not None. If some new parameters</span>
        <span class="c1"># are provided to the ask, the cache_ is not used.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">strategy</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span><span class="p">[(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">strategy</span><span class="p">)]</span>

        <span class="c1"># Copy of the optimizer is made in order to manage the</span>
        <span class="c1"># deletion of points with &quot;lie&quot; objective (the copy of</span>
        <span class="c1"># oiptimizer is simply discarded)</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
                                                      <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">))</span>

        <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">ti_available</span> <span class="o">=</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="n">ti</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">]</span> <span class="k">if</span> <span class="n">ti_available</span> <span class="k">else</span> <span class="bp">None</span>

            <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;cl_min&quot;</span><span class="p">:</span>
                <span class="n">y_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span> <span class="k">else</span> <span class="mf">0.0</span>  <span class="c1"># CL-min lie</span>
                <span class="n">t_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span> <span class="k">if</span> <span class="n">ti</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">log</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;cl_mean&quot;</span><span class="p">:</span>
                <span class="n">y_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span> <span class="k">else</span> <span class="mf">0.0</span>  <span class="c1"># CL-mean lie</span>
                <span class="n">t_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span> <span class="k">if</span> <span class="n">ti</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">log</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">y_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span> <span class="k">else</span> <span class="mf">0.0</span>  <span class="c1"># CL-max lie</span>
                <span class="n">t_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span> <span class="k">if</span> <span class="n">ti</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">log</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

            <span class="c1"># Lie to the optimizer.</span>
            <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">:</span>
                <span class="c1"># Use `_tell()` instead of `tell()` to prevent repeated</span>
                <span class="c1"># log transformations of the computation times.</span>
                <span class="n">opt</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y_lie</span><span class="p">,</span> <span class="n">t_lie</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">opt</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_lie</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="p">{(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span> <span class="n">X</span><span class="p">}</span>  <span class="c1"># cache_ the result</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_ask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Suggest next point at which to evaluate the objective.</span>

<span class="sd">        Return a random point while not at least `n_initial_points`</span>
<span class="sd">        observations have been `tell`ed, after that `base_estimator` is used</span>
<span class="sd">        to determine the next point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># this will not make a copy of `self.rng` and hence keep advancing</span>
            <span class="c1"># our random state.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Random evaluations exhausted and no &quot;</span>
                                   <span class="s2">&quot;model has been fit.&quot;</span><span class="p">)</span>

            <span class="n">next_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_x</span>
            <span class="n">min_delta_x</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">next_x</span><span class="p">,</span> <span class="n">xi</span><span class="p">)</span>
                               <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">])</span>
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">min_delta_x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1e-8</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The objective has been evaluated &quot;</span>
                              <span class="s2">&quot;at this point before.&quot;</span><span class="p">)</span>

            <span class="c1"># return point computed from last call to tell()</span>
            <span class="k">return</span> <span class="n">next_x</span>

    <span class="k">def</span> <span class="nf">tell</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Record an observation (or several) of the objective function.</span>

<span class="sd">        Provide values of the objective function at points suggested by `ask()`</span>
<span class="sd">        or other points. By default a new model will be fit to all</span>
<span class="sd">        observations. The new model is used to suggest the next point at</span>
<span class="sd">        which to evaluate the objective. This point can be retrieved by calling</span>
<span class="sd">        `ask()`.</span>

<span class="sd">        To add observations without fitting a new model set `fit` to False.</span>

<span class="sd">        To add multiple observations in a batch pass a list-of-lists for `x`</span>
<span class="sd">        and a list of scalars for `y`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `x` [list or list-of-lists]:</span>
<span class="sd">            Point at which objective was evaluated.</span>

<span class="sd">        * `y` [scalar or list]:</span>
<span class="sd">            Value of objective at `x`.</span>

<span class="sd">        * `fit` [bool, default=True]</span>
<span class="sd">            Fit a model to observed evaluations of the objective. A model will</span>
<span class="sd">            only be fitted after `n_initial_points` points have been told to</span>
<span class="sd">            the optimizer irrespective of the value of `fit`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_x_in_space</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_y_is_valid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># take the logarithm of the computation times</span>
        <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_2Dlistlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="n">val</span><span class="p">,</span> <span class="n">log</span><span class="p">(</span><span class="n">t</span><span class="p">)]</span> <span class="k">for</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">y</span><span class="p">]</span>
            <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="n">fit</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_tell</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform the actual work of incorporating one or more new points.</span>
<span class="sd">        See `tell()` for the full description.</span>

<span class="sd">        This method exists to give access to the internals of adding points</span>
<span class="sd">        by side stepping all input validation and transformation.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_2Dlistlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="c1"># if y isn&#39;t a scalar it means we have been handed a batch of points</span>
        <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_2Dlistlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Type of arguments `x` (</span><span class="si">%s</span><span class="s2">) and `y` (</span><span class="si">%s</span><span class="s2">) &quot;</span>
                             <span class="s2">&quot;not compatible.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>

        <span class="c1"># optimizer learned something new - discard cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># after being &quot;told&quot; n_initial_points we switch from sampling</span>
        <span class="c1"># random points to using a surrogate model</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">fit</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_initial_points</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">and</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">):</span>
            <span class="n">transformed_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">transformed_bounds</span><span class="p">)</span>
            <span class="n">est</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
                <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;next_xs_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="o">==</span> <span class="s2">&quot;gp_hedge&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gains_</span> <span class="o">-=</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_xs_</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est</span><span class="p">)</span>

            <span class="c1"># even with BFGS as optimizer we want to sample a large number</span>
            <span class="c1"># of points and then pick the best ones as starting points</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span>
                <span class="n">n_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_points</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">next_xs_</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">cand_acq_func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cand_acq_funcs_</span><span class="p">:</span>
                <span class="n">values</span> <span class="o">=</span> <span class="n">_gaussian_acquisition</span><span class="p">(</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">est</span><span class="p">,</span> <span class="n">y_opt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">),</span>
                    <span class="n">acq_func</span><span class="o">=</span><span class="n">cand_acq_func</span><span class="p">,</span>
                    <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func_kwargs</span><span class="p">)</span>
                <span class="c1"># Find the minimum of the acquisition function by randomly</span>
                <span class="c1"># sampling points from the space</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer</span> <span class="o">==</span> <span class="s2">&quot;sampling&quot;</span><span class="p">:</span>
                    <span class="n">next_x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">values</span><span class="p">)]</span>

                <span class="c1"># Use BFGS to find the mimimum of the acquisition function, the</span>
                <span class="c1"># minimization starts from `n_restarts_optimizer` different</span>
                <span class="c1"># points and the best minimum is used</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer</span> <span class="o">==</span> <span class="s2">&quot;lbfgs&quot;</span><span class="p">:</span>
                    <span class="n">x0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">values</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_restarts_optimizer</span><span class="p">]]</span>

                    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
                        <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
                            <span class="n">delayed</span><span class="p">(</span><span class="n">fmin_l_bfgs_b</span><span class="p">)(</span>
                                <span class="n">gaussian_acquisition_1D</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
                                <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">),</span> <span class="n">cand_acq_func</span><span class="p">,</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">acq_func_kwargs</span><span class="p">),</span>
                                <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">transformed_bounds</span><span class="p">,</span>
                                <span class="n">approx_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                <span class="n">maxiter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x0</span><span class="p">)</span>

                    <span class="n">cand_xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
                    <span class="n">cand_acqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
                    <span class="n">next_x</span> <span class="o">=</span> <span class="n">cand_xs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">cand_acqs</span><span class="p">)]</span>

                <span class="c1"># lbfgs should handle this but just in case there are</span>
                <span class="c1"># precision errors.</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">is_categorical</span><span class="p">:</span>
                    <span class="n">next_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                        <span class="n">next_x</span><span class="p">,</span> <span class="n">transformed_bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="n">transformed_bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">next_xs_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_x</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="o">==</span> <span class="s2">&quot;gp_hedge&quot;</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gains_</span><span class="p">)</span>
                <span class="n">logits</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
                <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">logits</span><span class="p">)</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">exp_logits</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">)</span>
                <span class="n">next_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_xs_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                                                      <span class="n">probs</span><span class="p">))]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_xs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># note the need for [0] at the end</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_next_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span>
                <span class="n">next_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Pack results</span>
        <span class="k">return</span> <span class="n">create_result</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="p">,</span>
                             <span class="n">models</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_y_is_valid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if the shape and types of x and y are consistent.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_2Dlistlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;expected y to be a list of (func_val, t)&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;expected y to be (func_val, t)&quot;</span><span class="p">)</span>

        <span class="c1"># if y isn&#39;t a scalar it means we have been handed a batch of points</span>
        <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_2Dlistlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">y_value</span> <span class="ow">in</span> <span class="n">y</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y_value</span><span class="p">,</span> <span class="n">Number</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected y to be a list of scalars&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Number</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`func` should return a scalar&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Type of arguments `x` (</span><span class="si">%s</span><span class="s2">) and `y` (</span><span class="si">%s</span><span class="s2">) &quot;</span>
                             <span class="s2">&quot;not compatible.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Execute ask() + tell() `n_iter` times&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">create_result</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="p">,</span>
                             <span class="n">models</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.Optimizer">Optimizer</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.Optimizer.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, dimensions, base_estimator=&#39;gp&#39;, n_random_starts=None, n_initial_points=10, acq_func=&#39;gp_hedge&#39;, acq_optimizer=&#39;auto&#39;, random_state=None, acq_func_kwargs=None, acq_optimizer_kwargs=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Optimizer.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Optimizer.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="s2">&quot;gp&quot;</span><span class="p">,</span>
             <span class="n">n_random_starts</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_initial_points</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
             <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;gp_hedge&quot;</span><span class="p">,</span>
             <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
             <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">acq_optimizer_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># Arguments that are just stored not checked</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="o">=</span> <span class="n">acq_func</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">acq_func_kwargs</span> <span class="o">=</span> <span class="n">acq_func_kwargs</span>
    <span class="n">allowed_acq_funcs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gp_hedge&quot;</span><span class="p">,</span> <span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="s2">&quot;LCB&quot;</span><span class="p">,</span> <span class="s2">&quot;PI&quot;</span><span class="p">,</span> <span class="s2">&quot;EIps&quot;</span><span class="p">,</span> <span class="s2">&quot;PIps&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_acq_funcs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected acq_func to be in </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">allowed_acq_funcs</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="o">==</span> <span class="s2">&quot;gp_hedge&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cand_acq_funcs_</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="s2">&quot;LCB&quot;</span><span class="p">,</span> <span class="s2">&quot;PI&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gains_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cand_acq_funcs_</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">acq_func_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">acq_func_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">acq_func_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eta&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">acq_optimizer_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">acq_optimizer_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_points</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_points&quot;</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_restarts_optimizer</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;n_restarts_optimizer&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">n_jobs</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer_kwargs</span> <span class="o">=</span> <span class="n">acq_optimizer_kwargs</span>
    <span class="k">if</span> <span class="n">n_random_starts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span><span class="s2">&quot;n_random_starts will be removed in favour of &quot;</span>
                       <span class="s2">&quot;n_initial_points.&quot;</span><span class="p">),</span>
                      <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="n">n_initial_points</span> <span class="o">=</span> <span class="n">n_random_starts</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_arguments</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">n_initial_points</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="p">,</span>
                          <span class="n">dimensions</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">,</span> <span class="n">GaussianProcessRegressor</span><span class="p">):</span>
        <span class="n">dimensions</span> <span class="o">=</span> <span class="n">normalize_dimensions</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">space</span> <span class="o">=</span> <span class="n">Space</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">yi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cat_inds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_non_cat_inds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensions</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cat_inds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_cat_inds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
    <span class="c1"># The cache of responses of `ask` method for n_points not None.</span>
    <span class="c1"># This ensures that multiple calls to `ask` with n_points set</span>
    <span class="c1"># return same sets of points.</span>
    <span class="c1"># The cache is reset to {} at every call to `tell`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Optimizer.ask">
    <p>def <span class="ident">ask</span>(</p><p>self, n_points=None, strategy=&#39;cl_min&#39;)</p>
    </div>
    

    
  
    <div class="desc"><p>Query point or multiple points at which objective should be evaluated.</p>
<ul>
<li>
<p><code>n_points</code> [int or None, default=None]:
    Number of points returned by the ask method.
    If the value is None, a single point to evaluate is returned.
    Otherwise a list of points to evaluate is returned of size
    n_points. This is useful if you can evaluate your objective in
    parallel, and thus obtain more objective function evaluations per
    unit of time.</p>
</li>
<li>
<p><code>strategy</code> [string, default=<code>"cl_min"</code>]:
    Method to use to sample multiple points (see also <code>n_points</code>
    description). This parameter is ignored if n_points = None.
    Supported options are <code>"cl_min"</code>, <code>"cl_mean"</code> or <code>"cl_max"</code>.</p>
<ul>
<li>If set to <code>"cl_min"</code>, then constant liar strtategy is used
   with lie objective value being minimum of observed objective
   values. <code>"cl_mean"</code> and <code>"cl_max"</code> means mean and max of values
   respectively. For details on this strategy see:</li>
</ul>
<p>https://hal.archives-ouvertes.fr/hal-00732512/document</p>
<p>With this strategy a copy of optimizer is created, which is
   then asked for a point, and the point is told to the copy of
   optimizer with some fake objective (lie), the next point is
   asked from copy, it is also told to the copy with fake
   objective and so on. The type of lie defines different
   flavours of <code>cl_x</code> strategies.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Optimizer.ask', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Optimizer.ask" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;cl_min&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Query point or multiple points at which objective should be evaluated.</span>
<span class="sd">    * `n_points` [int or None, default=None]:</span>
<span class="sd">        Number of points returned by the ask method.</span>
<span class="sd">        If the value is None, a single point to evaluate is returned.</span>
<span class="sd">        Otherwise a list of points to evaluate is returned of size</span>
<span class="sd">        n_points. This is useful if you can evaluate your objective in</span>
<span class="sd">        parallel, and thus obtain more objective function evaluations per</span>
<span class="sd">        unit of time.</span>
<span class="sd">    * `strategy` [string, default=`&quot;cl_min&quot;`]:</span>
<span class="sd">        Method to use to sample multiple points (see also `n_points`</span>
<span class="sd">        description). This parameter is ignored if n_points = None.</span>
<span class="sd">        Supported options are `&quot;cl_min&quot;`, `&quot;cl_mean&quot;` or `&quot;cl_max&quot;`.</span>
<span class="sd">        - If set to `&quot;cl_min&quot;`, then constant liar strtategy is used</span>
<span class="sd">           with lie objective value being minimum of observed objective</span>
<span class="sd">           values. `&quot;cl_mean&quot;` and `&quot;cl_max&quot;` means mean and max of values</span>
<span class="sd">           respectively. For details on this strategy see:</span>
<span class="sd">           https://hal.archives-ouvertes.fr/hal-00732512/document</span>
<span class="sd">           With this strategy a copy of optimizer is created, which is</span>
<span class="sd">           then asked for a point, and the point is told to the copy of</span>
<span class="sd">           optimizer with some fake objective (lie), the next point is</span>
<span class="sd">           asked from copy, it is also told to the copy with fake</span>
<span class="sd">           objective and so on. The type of lie defines different</span>
<span class="sd">           flavours of `cl_x` strategies.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n_points</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ask</span><span class="p">()</span>
    <span class="n">supported_strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cl_min&quot;</span><span class="p">,</span> <span class="s2">&quot;cl_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;cl_max&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">n_points</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;n_points should be int &gt; 0, got &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_points</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_strategies</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Expected parallel_strategy to be one of &quot;</span> <span class="o">+</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">supported_strategies</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span> <span class="o">+</span> <span class="s2">&quot;got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">strategy</span>
        <span class="p">)</span>
    <span class="c1"># Caching the result with n_points not None. If some new parameters</span>
    <span class="c1"># are provided to the ask, the cache_ is not used.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">strategy</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span><span class="p">[(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">strategy</span><span class="p">)]</span>
    <span class="c1"># Copy of the optimizer is made in order to manage the</span>
    <span class="c1"># deletion of points with &quot;lie&quot; objective (the copy of</span>
    <span class="c1"># oiptimizer is simply discarded)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
                                                  <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ti_available</span> <span class="o">=</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">ti</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">]</span> <span class="k">if</span> <span class="n">ti_available</span> <span class="k">else</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;cl_min&quot;</span><span class="p">:</span>
            <span class="n">y_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span> <span class="k">else</span> <span class="mf">0.0</span>  <span class="c1"># CL-min lie</span>
            <span class="n">t_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span> <span class="k">if</span> <span class="n">ti</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">log</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;cl_mean&quot;</span><span class="p">:</span>
            <span class="n">y_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span> <span class="k">else</span> <span class="mf">0.0</span>  <span class="c1"># CL-mean lie</span>
            <span class="n">t_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span> <span class="k">if</span> <span class="n">ti</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">log</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span> <span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">yi</span> <span class="k">else</span> <span class="mf">0.0</span>  <span class="c1"># CL-max lie</span>
            <span class="n">t_lie</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span> <span class="k">if</span> <span class="n">ti</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">log</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
        <span class="c1"># Lie to the optimizer.</span>
        <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">:</span>
            <span class="c1"># Use `_tell()` instead of `tell()` to prevent repeated</span>
            <span class="c1"># log transformations of the computation times.</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y_lie</span><span class="p">,</span> <span class="n">t_lie</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_lie</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="p">{(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span> <span class="n">X</span><span class="p">}</span>  <span class="c1"># cache_ the result</span>
    <span class="k">return</span> <span class="n">X</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Optimizer.copy">
    <p>def <span class="ident">copy</span>(</p><p>self, random_state=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Create a shallow copy of an instance of the optimizer.</p>
<h2>Parameters</h2>
<ul>
<li><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set the random state of the copy.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Optimizer.copy', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Optimizer.copy" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create a shallow copy of an instance of the optimizer.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set the random state of the copy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span>
        <span class="n">dimensions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensions</span><span class="p">,</span>
        <span class="n">base_estimator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">,</span>
        <span class="n">n_initial_points</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_initial_points_</span><span class="p">,</span>
        <span class="n">acq_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">,</span>
        <span class="n">acq_optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer</span><span class="p">,</span>
        <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_func_kwargs</span><span class="p">,</span>
        <span class="n">acq_optimizer_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acq_optimizer_kwargs</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gains_&quot;</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">gains_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gains_</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimizer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Optimizer.run">
    <p>def <span class="ident">run</span>(</p><p>self, func, n_iter=1)</p>
    </div>
    

    
  
    <div class="desc"><p>Execute ask() + tell() <code>n_iter</code> times</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Optimizer.run', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Optimizer.run" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Execute ask() + tell() `n_iter` times&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">create_result</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="p">,</span>
                         <span class="n">models</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Optimizer.tell">
    <p>def <span class="ident">tell</span>(</p><p>self, x, y, fit=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Record an observation (or several) of the objective function.</p>
<p>Provide values of the objective function at points suggested by <code>ask()</code>
or other points. By default a new model will be fit to all
observations. The new model is used to suggest the next point at
which to evaluate the objective. This point can be retrieved by calling
<code>ask()</code>.</p>
<p>To add observations without fitting a new model set <code>fit</code> to False.</p>
<p>To add multiple observations in a batch pass a list-of-lists for <code>x</code>
and a list of scalars for <code>y</code>.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>x</code> [list or list-of-lists]:
    Point at which objective was evaluated.</p>
</li>
<li>
<p><code>y</code> [scalar or list]:
    Value of objective at <code>x</code>.</p>
</li>
<li>
<p><code>fit</code> [bool, default=True]
    Fit a model to observed evaluations of the objective. A model will
    only be fitted after <code>n_initial_points</code> points have been told to
    the optimizer irrespective of the value of <code>fit</code>.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Optimizer.tell', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Optimizer.tell" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">tell</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Record an observation (or several) of the objective function.</span>
<span class="sd">    Provide values of the objective function at points suggested by `ask()`</span>
<span class="sd">    or other points. By default a new model will be fit to all</span>
<span class="sd">    observations. The new model is used to suggest the next point at</span>
<span class="sd">    which to evaluate the objective. This point can be retrieved by calling</span>
<span class="sd">    `ask()`.</span>
<span class="sd">    To add observations without fitting a new model set `fit` to False.</span>
<span class="sd">    To add multiple observations in a batch pass a list-of-lists for `x`</span>
<span class="sd">    and a list of scalars for `y`.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `x` [list or list-of-lists]:</span>
<span class="sd">        Point at which objective was evaluated.</span>
<span class="sd">    * `y` [scalar or list]:</span>
<span class="sd">        Value of objective at `x`.</span>
<span class="sd">    * `fit` [bool, default=True]</span>
<span class="sd">        Fit a model to observed evaluations of the objective. A model will</span>
<span class="sd">        only be fitted after `n_initial_points` points have been told to</span>
<span class="sd">        the optimizer irrespective of the value of `fit`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_x_in_space</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_y_is_valid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># take the logarithm of the computation times</span>
    <span class="k">if</span> <span class="s2">&quot;ps&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">acq_func</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_2Dlistlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="n">val</span><span class="p">,</span> <span class="n">log</span><span class="p">(</span><span class="n">t</span><span class="p">)]</span> <span class="k">for</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">y</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">is_listlike</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="n">fit</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.Optimizer.Xi" class="name">var <span class="ident">Xi</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.acq_func" class="name">var <span class="ident">acq_func</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.acq_func_kwargs" class="name">var <span class="ident">acq_func_kwargs</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.acq_optimizer_kwargs" class="name">var <span class="ident">acq_optimizer_kwargs</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.cache_" class="name">var <span class="ident">cache_</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.eta" class="name">var <span class="ident">eta</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.models" class="name">var <span class="ident">models</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.n_jobs" class="name">var <span class="ident">n_jobs</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.n_points" class="name">var <span class="ident">n_points</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.n_restarts_optimizer" class="name">var <span class="ident">n_restarts_optimizer</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.rng" class="name">var <span class="ident">rng</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.space" class="name">var <span class="ident">space</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Optimizer.yi" class="name">var <span class="ident">yi</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.Space" class="name">class <span class="ident">Space</span></p>
      
  
    <div class="desc"><p>Search space.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Space', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Space" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Space</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Search space.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize a search space from given specifications.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">            List of search space dimensions.</span>
<span class="sd">            Each search dimension can be defined either as</span>

<span class="sd">            - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">              dimensions),</span>
<span class="sd">            - a `(lower_bound, upper_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">              dimensions),</span>
<span class="sd">            - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">            - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">              `Categorical`).</span>

<span class="sd">            NOTE: The upper and lower bounds are inclusive for `Integer`</span>
<span class="sd">            dimensions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="n">check_dimension</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">([</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dimensions</span><span class="p">)])</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">31</span><span class="p">:</span>
            <span class="n">dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">[:</span><span class="mi">15</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">_Ellipsis</span><span class="p">()]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">[</span><span class="o">-</span><span class="mi">15</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span>
        <span class="k">return</span> <span class="s2">&quot;Space([{}])&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;,</span><span class="se">\n</span><span class="s1">       &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">dims</span><span class="p">)))</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_real</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns true if all dimensions are Real</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">Real</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Draw random samples.</span>

<span class="sd">        The samples are in the original space. They need to be transformed</span>
<span class="sd">        before being passed to a model or minimizer by `space.transform()`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `n_samples` [int, default=1]:</span>
<span class="sd">            Number of samples to be drawn from the space.</span>

<span class="sd">        * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">            Set random state to something other than None for reproducible</span>
<span class="sd">            results.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `points`: [list of lists, shape=(n_points, n_dims)]</span>
<span class="sd">           Points sampled from the space.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Draw</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sp_version</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
                <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

        <span class="c1"># Transpose</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
                <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>

            <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">rows</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform samples from the original space into a warped space.</span>

<span class="sd">        Note: this transformation is expected to be used to project samples</span>
<span class="sd">              into a suitable space for numerical optimization.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [list of lists, shape=(n_samples, n_dims)]:</span>
<span class="sd">            The samples to transform.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `Xt` [array of floats, shape=(n_samples, transformed_n_dims)]</span>
<span class="sd">            The transformed samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Pack by dimension</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">:</span>
            <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
                <span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>

        <span class="c1"># Transform</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
            <span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

        <span class="c1"># Repack as an array</span>
        <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">Xt</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Inverse transform samples from the warped space back to the</span>
<span class="sd">           original space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `Xt` [array of floats, shape=(n_samples, transformed_n_dims)]:</span>
<span class="sd">            The samples to inverse transform.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `X` [list of lists, shape=(n_samples, n_dims)]</span>
<span class="sd">            The original samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Inverse transform</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="n">dim</span><span class="o">.</span><span class="n">transformed_size</span>

            <span class="k">if</span> <span class="n">offset</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">[:,</span> <span class="n">start</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">dim</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:</span><span class="n">start</span><span class="o">+</span><span class="n">offset</span><span class="p">]))</span>

            <span class="n">start</span> <span class="o">+=</span> <span class="n">offset</span>

        <span class="c1"># Transpose</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xt</span><span class="p">)):</span>
            <span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
                <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>

            <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">rows</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The dimensionality of the original space.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">transformed_n_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The dimensionality of the warped space.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">dim</span><span class="o">.</span><span class="n">transformed_size</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bounds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The dimension bounds, in the original space.&quot;&quot;&quot;</span>
        <span class="n">b</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dim</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">b</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">b</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">b</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check that `point` is within the bounds of the space.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">component</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dim</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">transformed_bounds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The dimension bounds, in the warped space.&quot;&quot;&quot;</span>
        <span class="n">b</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dim</span><span class="o">.</span><span class="n">transformed_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">b</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">transformed_bounds</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">b</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">transformed_bounds</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">b</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_categorical</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Space contains exclusively categorical dimensions&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute distance between two points in this space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `a` [array]</span>
<span class="sd">            First point.</span>

<span class="sd">        * `b` [array]</span>
<span class="sd">            Second point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">):</span>
            <span class="n">distance</span> <span class="o">+=</span> <span class="n">dim</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">distance</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.Space">Space</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.Space.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, dimensions)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize a search space from given specifications.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(lower_bound, upper_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(lower_bound, upper_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
<p>NOTE: The upper and lower bounds are inclusive for <code>Integer</code>
dimensions.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Space.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Space.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize a search space from given specifications.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>
<span class="sd">        - a `(lower_bound, upper_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(lower_bound, upper_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>
<span class="sd">        NOTE: The upper and lower bounds are inclusive for `Integer`</span>
<span class="sd">        dimensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="n">check_dimension</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Space.distance">
    <p>def <span class="ident">distance</span>(</p><p>self, point_a, point_b)</p>
    </div>
    

    
  
    <div class="desc"><p>Compute distance between two points in this space.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>a</code> [array]
    First point.</p>
</li>
<li>
<p><code>b</code> [array]
    Second point.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Space.distance', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Space.distance" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute distance between two points in this space.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `a` [array]</span>
<span class="sd">        First point.</span>
<span class="sd">    * `b` [array]</span>
<span class="sd">        Second point.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">):</span>
        <span class="n">distance</span> <span class="o">+=</span> <span class="n">dim</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">distance</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Space.inverse_transform">
    <p>def <span class="ident">inverse_transform</span>(</p><p>self, Xt)</p>
    </div>
    

    
  
    <div class="desc"><p>Inverse transform samples from the warped space back to the
   original space.</p>
<h2>Parameters</h2>
<ul>
<li><code>Xt</code> [array of floats, shape=(n_samples, transformed_n_dims)]:
    The samples to inverse transform.</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>X</code> [list of lists, shape=(n_samples, n_dims)]
    The original samples.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Space.inverse_transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Space.inverse_transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inverse transform samples from the warped space back to the</span>
<span class="sd">       original space.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `Xt` [array of floats, shape=(n_samples, transformed_n_dims)]:</span>
<span class="sd">        The samples to inverse transform.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `X` [list of lists, shape=(n_samples, n_dims)]</span>
<span class="sd">        The original samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Inverse transform</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">offset</span> <span class="o">=</span> <span class="n">dim</span><span class="o">.</span><span class="n">transformed_size</span>
        <span class="k">if</span> <span class="n">offset</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">[:,</span> <span class="n">start</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:</span><span class="n">start</span><span class="o">+</span><span class="n">offset</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">offset</span>
    <span class="c1"># Transpose</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xt</span><span class="p">)):</span>
        <span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
            <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
        <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rows</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Space.rvs">
    <p>def <span class="ident">rvs</span>(</p><p>self, n_samples=1, random_state=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Draw random samples.</p>
<p>The samples are in the original space. They need to be transformed
before being passed to a model or minimizer by <code>space.transform()</code>.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>n_samples</code> [int, default=1]:
    Number of samples to be drawn from the space.</p>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>points</code>: [list of lists, shape=(n_points, n_dims)]
   Points sampled from the space.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Space.rvs', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Space.rvs" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Draw random samples.</span>
<span class="sd">    The samples are in the original space. They need to be transformed</span>
<span class="sd">    before being passed to a model or minimizer by `space.transform()`.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `n_samples` [int, default=1]:</span>
<span class="sd">        Number of samples to be drawn from the space.</span>
<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `points`: [list of lists, shape=(n_points, n_dims)]</span>
<span class="sd">       Points sampled from the space.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="c1"># Draw</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sp_version</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
            <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>
    <span class="c1"># Transpose</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
            <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
        <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rows</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.Space.transform">
    <p>def <span class="ident">transform</span>(</p><p>self, X)</p>
    </div>
    

    
  
    <div class="desc"><p>Transform samples from the original space into a warped space.</p>
<p>Note: this transformation is expected to be used to project samples
      into a suitable space for numerical optimization.</p>
<h2>Parameters</h2>
<ul>
<li><code>X</code> [list of lists, shape=(n_samples, n_dims)]:
    The samples to transform.</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>Xt</code> [array of floats, shape=(n_samples, transformed_n_dims)]
    The transformed samples.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.Space.transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.Space.transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform samples from the original space into a warped space.</span>
<span class="sd">    Note: this transformation is expected to be used to project samples</span>
<span class="sd">          into a suitable space for numerical optimization.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [list of lists, shape=(n_samples, n_dims)]:</span>
<span class="sd">        The samples to transform.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `Xt` [array of floats, shape=(n_samples, transformed_n_dims)]</span>
<span class="sd">        The transformed samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Pack by dimension</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">:</span>
        <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
            <span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
    <span class="c1"># Transform</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dims</span><span class="p">):</span>
        <span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensions</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="c1"># Repack as an array</span>
    <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">Xt</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.Space.bounds" class="name">var <span class="ident">bounds</span></p>
            

            
  
    <div class="desc"><p>The dimension bounds, in the original space.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Space.dimensions" class="name">var <span class="ident">dimensions</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Space.is_categorical" class="name">var <span class="ident">is_categorical</span></p>
            

            
  
    <div class="desc"><p>Space contains exclusively categorical dimensions</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Space.is_real" class="name">var <span class="ident">is_real</span></p>
            

            
  
    <div class="desc"><p>Returns true if all dimensions are Real</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Space.n_dims" class="name">var <span class="ident">n_dims</span></p>
            

            
  
    <div class="desc"><p>The dimensionality of the original space.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Space.transformed_bounds" class="name">var <span class="ident">transformed_bounds</span></p>
            

            
  
    <div class="desc"><p>The dimension bounds, in the warped space.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.Space.transformed_n_dims" class="name">var <span class="ident">transformed_n_dims</span></p>
            

            
  
    <div class="desc"><p>The dimensionality of the warped space.</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
