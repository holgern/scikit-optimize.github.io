

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>skopt’s top level minimization functions &mdash; scikit-optimize v0.6+61.g02ce7ba documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="skopt.Optimizer, ask-and-tell interface" href="optimizer.html" />
    <link rel="prev" title="API Reference" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> scikit-optimize
          

          
          </a>

          
            
            
              <div class="version">
                v0.6+61.g02ce7ba
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development.html">Development</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#"><code class="docutils literal notranslate"><span class="pre">skopt</span></code>’s top level minimization functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer.html"><code class="docutils literal notranslate"><span class="pre">skopt.Optimizer</span></code>, ask-and-tell interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayessearchcv.html"><code class="docutils literal notranslate"><span class="pre">skopt.BayesSearchCV</span></code>, GridSearchCV compatible estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="acquisition.html"><code class="docutils literal notranslate"><span class="pre">skopt.acquisition</span></code>, acquisition functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="callbacks.html"><code class="docutils literal notranslate"><span class="pre">skopt.callbacks</span></code>, callback functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="plots.html"><code class="docutils literal notranslate"><span class="pre">skopt.plots</span></code>, plotting tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="space.html"><code class="docutils literal notranslate"><span class="pre">skopt.space</span></code>, define the optimization space</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html"><code class="docutils literal notranslate"><span class="pre">skopt.utils</span></code>, utility functions</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">scikit-optimize</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">API Reference</a> &raquo;</li>
        
      <li><code class="docutils literal notranslate"><span class="pre">skopt</span></code>’s top level minimization functions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/reference/minimize_functions.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="skopt-s-top-level-minimization-functions">
<span id="minimize-functions"></span><h1><code class="docutils literal notranslate"><span class="pre">skopt</span></code>’s top level minimization functions<a class="headerlink" href="#skopt-s-top-level-minimization-functions" title="Permalink to this headline">¶</a></h1>
<p>These are easy to get started with. They mirror the <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code>
API and provide a high level interface to various pre-configured
optimizers.</p>
<dl class="function">
<dt id="skopt.dummy_minimize">
<code class="sig-prename descclassname">skopt.</code><code class="sig-name descname">dummy_minimize</code><span class="sig-paren">(</span><em class="sig-param">func</em>, <em class="sig-param">dimensions</em>, <em class="sig-param">n_calls=100</em>, <em class="sig-param">x0=None</em>, <em class="sig-param">y0=None</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">callback=None</em>, <em class="sig-param">model_queue_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skopt/optimizer/dummy.html#dummy_minimize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skopt.dummy_minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Random search by uniform sampling within the given bounds.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong><strong> [</strong><strong>callable</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize. Should take a single list of parameters
and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <cite>skopt.utils.use_named_args</cite> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <cite>use_named_args</cite> for an example.</p>
</p></li>
<li><p><strong>dimensions</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>shape=</strong><strong>(</strong><strong>n_dims</strong><strong>,</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – <p>List of search space dimensions.
Each search dimension can be defined either as</p>
<ul>
<li><p>a <cite>(lower_bound, upper_bound)</cite> tuple (for <cite>Real</cite> or <cite>Integer</cite>
dimensions),</p></li>
<li><p>a <cite>(lower_bound, upper_bound, prior)</cite> tuple (for <cite>Real</cite>
dimensions),</p></li>
<li><p>as a list of categories (for <cite>Categorical</cite> dimensions), or</p></li>
<li><p>an instance of a <cite>Dimension</cite> object (<cite>Real</cite>, <cite>Integer</cite> or
<cite>Categorical</cite>).</p></li>
</ul>
</p></li>
<li><p><strong>n_calls</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=100</strong><strong>]</strong> (<em>*</em>) – Number of calls to <cite>func</cite> to find the minimum.</p></li>
<li><p><strong>x0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>list of lists</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Initial input points.</p>
<ul>
<li><p>If it is a list of lists, use it as a list of input points.</p></li>
<li><p>If it is a list, use it as a single initial input point.</p></li>
<li><p>If it is <cite>None</cite>, no initial input points are used.</p></li>
</ul>
</p></li>
<li><p><strong>y0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>scalar</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Evaluation of initial input points.</p>
<ul>
<li><p>If it is a list, then it corresponds to evaluations of the function
at each element of <cite>x0</cite> : the i-th element of <cite>y0</cite> corresponds
to the function evaluated at the i-th element of <cite>x0</cite>.</p></li>
<li><p>If it is a scalar, then it corresponds to the evaluation of the
function at <cite>x0</cite>.</p></li>
<li><p>If it is None and <cite>x0</cite> is provided, then the function is evaluated
at each element of <cite>x0</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>random_state</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>RandomState instance</strong><strong>, or </strong><strong>None</strong><strong> (</strong><strong>default</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – Set random state to something other than None for reproducible
results.</p></li>
<li><p><strong>verbose</strong><strong> [</strong><strong>boolean</strong><strong>, </strong><strong>default=False</strong><strong>]</strong> (<em>*</em>) – Control the verbosity. It is advised to set the verbosity to True
for long optimization runs.</p></li>
<li><p><strong>callback</strong><strong> [</strong><strong>callable</strong><strong>, </strong><strong>list of callables</strong><strong>, </strong><strong>optional</strong><strong>]</strong> (<em>*</em>) – If callable then <cite>callback(res)</cite> is called after each call to <cite>func</cite>.
If list of callables, then each callable in the list is called.</p></li>
<li><p><strong>model_queue_size</strong><strong> [</strong><strong>int</strong><strong> or </strong><strong>None</strong><strong>, </strong><strong>default=None</strong><strong>]</strong> (<em>*</em>) – Keeps list of models only as long as the argument given. In the
case of None, the list has no capped length.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The optimization result returned as a OptimizeResult object.
Important attributes are:</p>
<ul class="simple">
<li><p><cite>x</cite> [list]: location of the minimum.</p></li>
<li><p><cite>fun</cite> [float]: function value at the minimum.</p></li>
<li><dl class="simple">
<dt><cite>x_iters</cite> [list of lists]: location of function evaluation for each</dt><dd><p>iteration.</p>
</dd>
</dl>
</li>
<li><p><cite>func_vals</cite> [array]: function value for each iteration.</p></li>
<li><p><cite>space</cite> [Space]: the optimisation space.</p></li>
<li><p><cite>specs</cite> [dict]: the call specifications.</p></li>
<li><dl class="simple">
<dt><cite>rng</cite> [RandomState instance]: State of the random state</dt><dd><p>at the end of minimization.</p>
</dd>
</dl>
</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html">http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</a></p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>* <cite>res</cite> [<cite>OptimizeResult</cite>, scipy object]</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="skopt.forest_minimize">
<code class="sig-prename descclassname">skopt.</code><code class="sig-name descname">forest_minimize</code><span class="sig-paren">(</span><em class="sig-param">func</em>, <em class="sig-param">dimensions</em>, <em class="sig-param">base_estimator='ET'</em>, <em class="sig-param">n_calls=100</em>, <em class="sig-param">n_random_starts=10</em>, <em class="sig-param">acq_func='EI'</em>, <em class="sig-param">x0=None</em>, <em class="sig-param">y0=None</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">callback=None</em>, <em class="sig-param">n_points=10000</em>, <em class="sig-param">xi=0.01</em>, <em class="sig-param">kappa=1.96</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">model_queue_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skopt/optimizer/forest.html#forest_minimize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skopt.forest_minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequential optimisation using decision trees.</p>
<p>A tree based regression model is used to model the expensive to evaluate
function <cite>func</cite>. The model is improved by sequentially evaluating
the expensive function at the next best point. Thereby finding the
minimum of <cite>func</cite> with as few evaluations as possible.</p>
<p>The total number of evaluations, <cite>n_calls</cite>, are performed like the
following. If <cite>x0</cite> is provided but not <cite>y0</cite>, then the elements of <cite>x0</cite>
are first evaluated, followed by <cite>n_random_starts</cite> evaluations.
Finally, <cite>n_calls - len(x0) - n_random_starts</cite> evaluations are
made guided by the surrogate model. If <cite>x0</cite> and <cite>y0</cite> are both
provided then <cite>n_random_starts</cite> evaluations are first made then
<cite>n_calls - n_random_starts</cite> subsequent evaluations are made
guided by the surrogate model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong><strong> [</strong><strong>callable</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize. Should take a single list of parameters
and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <cite>skopt.utils.use_named_args</cite> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <cite>use_named_args</cite> for an example.</p>
</p></li>
<li><p><strong>dimensions</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>shape=</strong><strong>(</strong><strong>n_dims</strong><strong>,</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – <p>List of search space dimensions.
Each search dimension can be defined either as</p>
<ul>
<li><p>a <cite>(lower_bound, upper_bound)</cite> tuple (for <cite>Real</cite> or <cite>Integer</cite>
dimensions),</p></li>
<li><p>a <cite>(lower_bound, upper_bound, prior)</cite> tuple (for <cite>Real</cite>
dimensions),</p></li>
<li><p>as a list of categories (for <cite>Categorical</cite> dimensions), or</p></li>
<li><p>an instance of a <cite>Dimension</cite> object (<cite>Real</cite>, <cite>Integer</cite> or
<cite>Categorical</cite>).</p></li>
</ul>
<blockquote>
<div><p>NOTE: The upper and lower bounds are inclusive for <cite>Integer</cite>
dimensions.</p>
</div></blockquote>
</p></li>
<li><p><strong>base_estimator</strong><strong> [</strong><strong>string</strong><strong> or </strong><strong>Regressor</strong><strong>, </strong><strong>default=`&quot;ET&quot;`</strong><strong>]</strong> (<em>*</em>) – <p>The regressor to use as surrogate model. Can be either</p>
<ul>
<li><p><cite>”RF”</cite> for random forest regressor</p></li>
<li><p><cite>”ET”</cite> for extra trees regressor</p></li>
<li><p>instance of regressor with support for <cite>return_std</cite> in its predict
method</p></li>
</ul>
<p>The predefined models are initilized with good defaults. If you
want to adjust the model parameters pass your own instance of
a regressor which returns the mean and standard deviation when
making predictions.</p>
</p></li>
<li><p><strong>n_calls</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=100</strong><strong>]</strong> (<em>*</em>) – Number of calls to <cite>func</cite>.</p></li>
<li><p><strong>n_random_starts</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=10</strong><strong>]</strong> (<em>*</em>) – Number of evaluations of <cite>func</cite> with random points before
approximating it with <cite>base_estimator</cite>.</p></li>
<li><p><strong>acq_func</strong><strong> [</strong><strong>string</strong><strong>, </strong><strong>default=`&quot;LCB&quot;`</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize over the forest posterior. Can be either</p>
<ul>
<li><p><cite>”LCB”</cite> for lower confidence bound.</p></li>
<li><p><cite>”EI”</cite> for negative expected improvement.</p></li>
<li><p><cite>”PI”</cite> for negative probability of improvement.</p></li>
<li><p><a href="#id5"><span class="problematic" id="id6">`</span></a>”EIps” for negated expected improvement per second to take into
account the function compute time. Then, the objective function is
assumed to return two values, the first being the objective value and
the second being the time taken in seconds.</p></li>
<li><p><cite>”PIps”</cite> for negated probability of improvement per second. The
return type of the objective function is assumed to be similar to
that of <cite>“EIps”</cite></p></li>
</ul>
</p></li>
<li><p><strong>x0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>list of lists</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Initial input points.</p>
<ul>
<li><p>If it is a list of lists, use it as a list of input points.</p></li>
<li><p>If it is a list, use it as a single initial input point.</p></li>
<li><p>If it is <cite>None</cite>, no initial input points are used.</p></li>
</ul>
</p></li>
<li><p><strong>y0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>scalar</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Evaluation of initial input points.</p>
<ul>
<li><p>If it is a list, then it corresponds to evaluations of the function
at each element of <cite>x0</cite> : the i-th element of <cite>y0</cite> corresponds
to the function evaluated at the i-th element of <cite>x0</cite>.</p></li>
<li><p>If it is a scalar, then it corresponds to the evaluation of the
function at <cite>x0</cite>.</p></li>
<li><p>If it is None and <cite>x0</cite> is provided, then the function is evaluated
at each element of <cite>x0</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>random_state</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>RandomState instance</strong><strong>, or </strong><strong>None</strong><strong> (</strong><strong>default</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – Set random state to something other than None for reproducible
results.</p></li>
<li><p><strong>verbose</strong><strong> [</strong><strong>boolean</strong><strong>, </strong><strong>default=False</strong><strong>]</strong> (<em>*</em>) – Control the verbosity. It is advised to set the verbosity to True
for long optimization runs.</p></li>
<li><p><strong>callback</strong><strong> [</strong><strong>callable</strong><strong>, </strong><strong>optional</strong><strong>]</strong> (<em>*</em>) – If provided, then <cite>callback(res)</cite> is called after call to func.</p></li>
<li><p><strong>n_points</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=10000</strong><strong>]</strong> (<em>*</em>) – Number of points to sample when minimizing the acquisition function.</p></li>
<li><p><strong>xi</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=0.01</strong><strong>]</strong> (<em>*</em>) – Controls how much improvement one wants over the previous best
values. Used when the acquisition is either <cite>“EI”</cite> or <cite>“PI”</cite>.</p></li>
<li><p><strong>kappa</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=1.96</strong><strong>]</strong> (<em>*</em>) – Controls how much of the variance in the predicted values should be
taken into account. If set to be very high, then we are favouring
exploration over exploitation and vice versa.
Used when the acquisition is <cite>“LCB”</cite>.</p></li>
<li><p><strong>n_jobs</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=1</strong><strong>]</strong> (<em>*</em>) – The number of jobs to run in parallel for <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</p></li>
<li><p><strong>model_queue_size</strong><strong> [</strong><strong>int</strong><strong> or </strong><strong>None</strong><strong>, </strong><strong>default=None</strong><strong>]</strong> (<em>*</em>) – Keeps list of models only as long as the argument given. In the
case of None, the list has no capped length.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The optimization result returned as a OptimizeResult object.
Important attributes are:</p>
<ul class="simple">
<li><p><cite>x</cite> [list]: location of the minimum.</p></li>
<li><p><cite>fun</cite> [float]: function value at the minimum.</p></li>
<li><p><cite>models</cite>: surrogate models used for each iteration.</p></li>
<li><dl class="simple">
<dt><cite>x_iters</cite> [list of lists]: location of function evaluation for each</dt><dd><p>iteration.</p>
</dd>
</dl>
</li>
<li><p><cite>func_vals</cite> [array]: function value for each iteration.</p></li>
<li><p><cite>space</cite> [Space]: the optimization space.</p></li>
<li><p><cite>specs</cite> [dict]`: the call specifications.</p></li>
</ul>
<p>For more details related to the OptimizeResult object, refer
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html">http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</a></p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>* <cite>res</cite> [<cite>OptimizeResult</cite>, scipy object]</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="skopt.gbrt_minimize">
<code class="sig-prename descclassname">skopt.</code><code class="sig-name descname">gbrt_minimize</code><span class="sig-paren">(</span><em class="sig-param">func</em>, <em class="sig-param">dimensions</em>, <em class="sig-param">base_estimator=None</em>, <em class="sig-param">n_calls=100</em>, <em class="sig-param">n_random_starts=10</em>, <em class="sig-param">acq_func='EI'</em>, <em class="sig-param">acq_optimizer='auto'</em>, <em class="sig-param">x0=None</em>, <em class="sig-param">y0=None</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">callback=None</em>, <em class="sig-param">n_points=10000</em>, <em class="sig-param">xi=0.01</em>, <em class="sig-param">kappa=1.96</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">model_queue_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skopt/optimizer/gbrt.html#gbrt_minimize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skopt.gbrt_minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequential optimization using gradient boosted trees.</p>
<p>Gradient boosted regression trees are used to model the (very)
expensive to evaluate function <cite>func</cite>. The model is improved
by sequentially evaluating the expensive function at the next
best point. Thereby finding the minimum of <cite>func</cite> with as
few evaluations as possible.</p>
<p>The total number of evaluations, <cite>n_calls</cite>, are performed like the
following. If <cite>x0</cite> is provided but not <cite>y0</cite>, then the elements of <cite>x0</cite>
are first evaluated, followed by <cite>n_random_starts</cite> evaluations.
Finally, <cite>n_calls - len(x0) - n_random_starts</cite> evaluations are
made guided by the surrogate model. If <cite>x0</cite> and <cite>y0</cite> are both
provided then <cite>n_random_starts</cite> evaluations are first made then
<cite>n_calls - n_random_starts</cite> subsequent evaluations are made
guided by the surrogate model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong><strong> [</strong><strong>callable</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize. Should take a single list of parameters
and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <cite>skopt.utils.use_named_args</cite> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <cite>use_named_args</cite> for an example.</p>
</p></li>
<li><p><strong>dimensions</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>shape=</strong><strong>(</strong><strong>n_dims</strong><strong>,</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – <p>List of search space dimensions.
Each search dimension can be defined either as</p>
<ul>
<li><p>a <cite>(lower_bound, upper_bound)</cite> tuple (for <cite>Real</cite> or <cite>Integer</cite>
dimensions),</p></li>
<li><p>a <cite>(lower_bound, upper_bound, “prior”)</cite> tuple (for <cite>Real</cite>
dimensions),</p></li>
<li><p>as a list of categories (for <cite>Categorical</cite> dimensions), or</p></li>
<li><p>an instance of a <cite>Dimension</cite> object (<cite>Real</cite>, <cite>Integer</cite> or
<cite>Categorical</cite>).</p></li>
</ul>
</p></li>
<li><p><strong>base_estimator</strong><strong> [</strong><strong>GradientBoostingQuantileRegressor</strong><strong>]</strong> (<em>*</em>) – The regressor to use as surrogate model</p></li>
<li><p><strong>n_calls</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=100</strong><strong>]</strong> (<em>*</em>) – Number of calls to <cite>func</cite>.</p></li>
<li><p><strong>n_random_starts</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=10</strong><strong>]</strong> (<em>*</em>) – Number of evaluations of <cite>func</cite> with random points before
approximating it with <cite>base_estimator</cite>.</p></li>
<li><p><strong>acq_func</strong><strong> [</strong><strong>string</strong><strong>, </strong><strong>default=`&quot;LCB&quot;`</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize over the forest posterior. Can be either</p>
<ul>
<li><p><cite>”LCB”</cite> for lower confidence bound.</p></li>
<li><p><cite>”EI”</cite> for negative expected improvement.</p></li>
<li><p><cite>”PI”</cite> for negative probability of improvement.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;EIps&quot;</span></code> for negated expected improvement per second to take into
account the function compute time. Then, the objective function is
assumed to return two values, the first being the objective value and
the second being the time taken.</p></li>
<li><p><cite>”PIps”</cite> for negated probability of improvement per second.</p></li>
</ul>
</p></li>
<li><p><strong>x0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>list of lists</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Initial input points.</p>
<ul>
<li><p>If it is a list of lists, use it as a list of input points.</p></li>
<li><p>If it is a list, use it as a single initial input point.</p></li>
<li><p>If it is <cite>None</cite>, no initial input points are used.</p></li>
</ul>
</p></li>
<li><p><strong>y0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>scalar</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Evaluation of initial input points.</p>
<ul>
<li><p>If it is a list, then it corresponds to evaluations of the function
at each element of <cite>x0</cite> : the i-th element of <cite>y0</cite> corresponds
to the function evaluated at the i-th element of <cite>x0</cite>.</p></li>
<li><p>If it is a scalar, then it corresponds to the evaluation of the
function at <cite>x0</cite>.</p></li>
<li><p>If it is None and <cite>x0</cite> is provided, then the function is evaluated
at each element of <cite>x0</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>random_state</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>RandomState instance</strong><strong>, or </strong><strong>None</strong><strong> (</strong><strong>default</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – Set random state to something other than None for reproducible
results.</p></li>
<li><p><strong>verbose</strong><strong> [</strong><strong>boolean</strong><strong>, </strong><strong>default=False</strong><strong>]</strong> (<em>*</em>) – Control the verbosity. It is advised to set the verbosity to True
for long optimization runs.</p></li>
<li><p><strong>callback</strong><strong> [</strong><strong>callable</strong><strong>, </strong><strong>optional</strong><strong>]</strong> (<em>*</em>) – If provided, then <cite>callback(res)</cite> is called after call to func.</p></li>
<li><p><strong>n_points</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=10000</strong><strong>]</strong> (<em>*</em>) – Number of points to sample when minimizing the acquisition function.</p></li>
<li><p><strong>xi</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=0.01</strong><strong>]</strong> (<em>*</em>) – Controls how much improvement one wants over the previous best
values. Used when the acquisition is either <cite>“EI”</cite> or <cite>“PI”</cite>.</p></li>
<li><p><strong>kappa</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=1.96</strong><strong>]</strong> (<em>*</em>) – Controls how much of the variance in the predicted values should be
taken into account. If set to be very high, then we are favouring
exploration over exploitation and vice versa.
Used when the acquisition is <cite>“LCB”</cite>.</p></li>
<li><p><strong>n_jobs</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=1</strong><strong>]</strong> (<em>*</em>) – The number of jobs to run in parallel for <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</p></li>
<li><p><strong>model_queue_size</strong><strong> [</strong><strong>int</strong><strong> or </strong><strong>None</strong><strong>, </strong><strong>default=None</strong><strong>]</strong> (<em>*</em>) – Keeps list of models only as long as the argument given. In the
case of None, the list has no capped length.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The optimization result returned as a OptimizeResult object.
Important attributes are:</p>
<ul class="simple">
<li><p><cite>x</cite> [list]: location of the minimum.</p></li>
<li><p><cite>fun</cite> [float]: function value at the minimum.</p></li>
<li><p><cite>models</cite>: surrogate models used for each iteration.</p></li>
<li><dl class="simple">
<dt><cite>x_iters</cite> [list of lists]: location of function evaluation for each</dt><dd><p>iteration.</p>
</dd>
</dl>
</li>
<li><p><cite>func_vals</cite> [array]: function value for each iteration.</p></li>
<li><p><cite>space</cite> [Space]: the optimization space.</p></li>
<li><p><cite>specs</cite> [dict]`: the call specifications.</p></li>
<li><dl class="simple">
<dt><cite>rng</cite> [RandomState instance]: State of the random state</dt><dd><p>at the end of minimization.</p>
</dd>
</dl>
</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html">http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</a></p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>* <cite>res</cite> [<cite>OptimizeResult</cite>, scipy object]</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="skopt.gp_minimize">
<code class="sig-prename descclassname">skopt.</code><code class="sig-name descname">gp_minimize</code><span class="sig-paren">(</span><em class="sig-param">func</em>, <em class="sig-param">dimensions</em>, <em class="sig-param">base_estimator=None</em>, <em class="sig-param">n_calls=100</em>, <em class="sig-param">n_random_starts=10</em>, <em class="sig-param">acq_func='gp_hedge'</em>, <em class="sig-param">acq_optimizer='auto'</em>, <em class="sig-param">x0=None</em>, <em class="sig-param">y0=None</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">callback=None</em>, <em class="sig-param">n_points=10000</em>, <em class="sig-param">n_restarts_optimizer=5</em>, <em class="sig-param">xi=0.01</em>, <em class="sig-param">kappa=1.96</em>, <em class="sig-param">noise='gaussian'</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">model_queue_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skopt/optimizer/gp.html#gp_minimize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skopt.gp_minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian optimization using Gaussian Processes.</p>
<p>If every function evaluation is expensive, for instance
when the parameters are the hyperparameters of a neural network
and the function evaluation is the mean cross-validation score across
ten folds, optimizing the hyperparameters by standard optimization
routines would take for ever!</p>
<p>The idea is to approximate the function using a Gaussian process.
In other words the function values are assumed to follow a multivariate
gaussian. The covariance of the function values are given by a
GP kernel between the parameters. Then a smart choice to choose the
next parameter to evaluate can be made by the acquisition function
over the Gaussian prior which is much quicker to evaluate.</p>
<p>The total number of evaluations, <cite>n_calls</cite>, are performed like the
following. If <cite>x0</cite> is provided but not <cite>y0</cite>, then the elements of <cite>x0</cite>
are first evaluated, followed by <cite>n_random_starts</cite> evaluations.
Finally, <cite>n_calls - len(x0) - n_random_starts</cite> evaluations are
made guided by the surrogate model. If <cite>x0</cite> and <cite>y0</cite> are both
provided then <cite>n_random_starts</cite> evaluations are first made then
<cite>n_calls - n_random_starts</cite> subsequent evaluations are made
guided by the surrogate model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong><strong> [</strong><strong>callable</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize. Should take a single list of parameters
and return the objective value.</p>
<p>If you have a search-space where all dimensions have names,
then you can use <cite>skopt.utils.use_named_args</cite> as a decorator
on your objective function, in order to call it directly
with the named arguments. See <cite>use_named_args</cite> for an example.</p>
</p></li>
<li><p><strong>dimensions</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>shape=</strong><strong>(</strong><strong>n_dims</strong><strong>,</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – <p>List of search space dimensions.
Each search dimension can be defined either as</p>
<ul>
<li><p>a <cite>(lower_bound, upper_bound)</cite> tuple (for <cite>Real</cite> or <cite>Integer</cite>
dimensions),</p></li>
<li><p>a <cite>(lower_bound, upper_bound, “prior”)</cite> tuple (for <cite>Real</cite>
dimensions),</p></li>
<li><p>as a list of categories (for <cite>Categorical</cite> dimensions), or</p></li>
<li><p>an instance of a <cite>Dimension</cite> object (<cite>Real</cite>, <cite>Integer</cite> or
<cite>Categorical</cite>).</p></li>
</ul>
<blockquote>
<div><p>NOTE: The upper and lower bounds are inclusive for <cite>Integer</cite>
dimensions.</p>
</div></blockquote>
</p></li>
<li><p><strong>base_estimator</strong><strong> [</strong><strong>a Gaussian process estimator</strong><strong>]</strong> (<em>*</em>) – <p>The Gaussian process estimator to use for optimization.
By default, a Matern kernel is used with the following
hyperparameters tuned.
- All the length scales of the Matern kernel.
- The covariance amplitude that each element is multiplied with.
- Noise that is added to the matern kernel. The noise is assumed</p>
<blockquote>
<div><p>to be iid gaussian.</p>
</div></blockquote>
</p></li>
<li><p><strong>n_calls</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=100</strong><strong>]</strong> (<em>*</em>) – Number of calls to <cite>func</cite>.</p></li>
<li><p><strong>n_random_starts</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=10</strong><strong>]</strong> (<em>*</em>) – Number of evaluations of <cite>func</cite> with random points before
approximating it with <cite>base_estimator</cite>.</p></li>
<li><p><strong>acq_func</strong><strong> [</strong><strong>string</strong><strong>, </strong><strong>default=`&quot;gp_hedge&quot;`</strong><strong>]</strong> (<em>*</em>) – <p>Function to minimize over the gaussian prior. Can be either</p>
<ul>
<li><p><cite>”LCB”</cite> for lower confidence bound.</p></li>
<li><p><cite>”EI”</cite> for negative expected improvement.</p></li>
<li><p><cite>”PI”</cite> for negative probability of improvement.</p></li>
<li><p><cite>”gp_hedge”</cite> Probabilistically choose one of the above three
acquisition functions at every iteration. The weightage
given to these gains can be set by <cite>eta</cite> through <cite>acq_func_kwargs</cite>.</p>
<blockquote>
<div><ul class="simple">
<li><p>The gains <cite>g_i</cite> are initialized to zero.</p></li>
<li><dl class="simple">
<dt>At every iteration,</dt><dd><ul>
<li><p>Each acquisition function is optimised independently to
propose an candidate point <cite>X_i</cite>.</p></li>
<li><p>Out of all these candidate points, the next point <cite>X_best</cite> is
chosen by <cite>softmax(eta g_i)</cite></p></li>
<li><p>After fitting the surrogate model with <cite>(X_best, y_best)</cite>,
the gains are updated such that <cite>g_i -= mu(X_i)</cite></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
<li><p><cite>”EIps”</cite> for negated expected improvement per second to take into
account the function compute time. Then, the objective function is
assumed to return two values, the first being the objective value and
the second being the time taken in seconds.</p></li>
<li><p><cite>”PIps”</cite> for negated probability of improvement per second. The
return type of the objective function is assumed to be similar to
that of <a href="#id11"><span class="problematic" id="id12">`</span></a>”EIps</p></li>
</ul>
</p></li>
<li><p><strong>acq_optimizer</strong><strong> [</strong><strong>string</strong><strong>, </strong><strong>&quot;sampling&quot;</strong><strong> or </strong><strong>&quot;lbfgs&quot;</strong><strong>, </strong><strong>default=`&quot;lbfgs&quot;`</strong><strong>]</strong> (<em>*</em>) – <p>Method to minimize the acquistion function. The fit model
is updated with the optimal value obtained by optimizing <cite>acq_func</cite>
with <cite>acq_optimizer</cite>.</p>
<p>The <cite>acq_func</cite> is computed at <cite>n_points</cite> sampled randomly.</p>
<ul>
<li><p>If set to <cite>“auto”</cite>, then <cite>acq_optimizer</cite> is configured on the
basis of the space searched over.
If the space is Categorical then this is set to be “sampling”<a href="#id15"><span class="problematic" id="id16">`</span></a>.</p></li>
<li><p>If set to <cite>“sampling”</cite>, then the point among these <cite>n_points</cite>
where the <cite>acq_func</cite> is minimum is the next candidate minimum.</p></li>
<li><dl class="simple">
<dt>If set to <cite>“lbfgs”</cite>, then</dt><dd><ul>
<li><p>The <cite>n_restarts_optimizer</cite> no. of points which the acquisition
function is least are taken as start points.</p></li>
<li><p><cite>”lbfgs”</cite> is run for 20 iterations with these points as initial
points to find local minima.</p></li>
<li><p>The optimal of these local minima is used to update the prior.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</p></li>
<li><p><strong>x0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>list of lists</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Initial input points.</p>
<ul>
<li><p>If it is a list of lists, use it as a list of input points.</p></li>
<li><p>If it is a list, use it as a single initial input point.</p></li>
<li><p>If it is <cite>None</cite>, no initial input points are used.</p></li>
</ul>
</p></li>
<li><p><strong>y0</strong><strong> [</strong><strong>list</strong><strong>, </strong><strong>scalar</strong><strong> or </strong><strong>None</strong><strong>]</strong> (<em>*</em>) – <p>Evaluation of initial input points.</p>
<ul>
<li><p>If it is a list, then it corresponds to evaluations of the function
at each element of <cite>x0</cite> : the i-th element of <cite>y0</cite> corresponds
to the function evaluated at the i-th element of <cite>x0</cite>.</p></li>
<li><p>If it is a scalar, then it corresponds to the evaluation of the
function at <cite>x0</cite>.</p></li>
<li><p>If it is None and <cite>x0</cite> is provided, then the function is evaluated
at each element of <cite>x0</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>random_state</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>RandomState instance</strong><strong>, or </strong><strong>None</strong><strong> (</strong><strong>default</strong><strong>)</strong><strong>]</strong> (<em>*</em>) – Set random state to something other than None for reproducible
results.</p></li>
<li><p><strong>verbose</strong><strong> [</strong><strong>boolean</strong><strong>, </strong><strong>default=False</strong><strong>]</strong> (<em>*</em>) – Control the verbosity. It is advised to set the verbosity to True
for long optimization runs.</p></li>
<li><p><strong>callback</strong><strong> [</strong><strong>callable</strong><strong>, </strong><strong>list of callables</strong><strong>, </strong><strong>optional</strong><strong>]</strong> (<em>*</em>) – If callable then <cite>callback(res)</cite> is called after each call to <cite>func</cite>.
If list of callables, then each callable in the list is called.</p></li>
<li><p><strong>n_points</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=10000</strong><strong>]</strong> (<em>*</em>) – Number of points to sample to determine the next “best” point.
Useless if acq_optimizer is set to <cite>“lbfgs”</cite>.</p></li>
<li><p><strong>n_restarts_optimizer</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=5</strong><strong>]</strong> (<em>*</em>) – The number of restarts of the optimizer when <cite>acq_optimizer</cite>
is <cite>“lbfgs”</cite>.</p></li>
<li><p><strong>kappa</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=1.96</strong><strong>]</strong> (<em>*</em>) – Controls how much of the variance in the predicted values should be
taken into account. If set to be very high, then we are favouring
exploration over exploitation and vice versa.
Used when the acquisition is <cite>“LCB”</cite>.</p></li>
<li><p><strong>xi</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=0.01</strong><strong>]</strong> (<em>*</em>) – Controls how much improvement one wants over the previous best
values. Used when the acquisition is either <cite>“EI”</cite> or <cite>“PI”</cite>.</p></li>
<li><p><strong>noise</strong><strong> [</strong><strong>float</strong><strong>, </strong><strong>default=&quot;gaussian&quot;</strong><strong>]</strong> (<em>*</em>) – <ul>
<li><p>Use noise=”gaussian” if the objective returns noisy observations.
The noise of each observation is assumed to be iid with
mean zero and a fixed variance.</p></li>
<li><p>If the variance is known before-hand, this can be set directly
to the variance of the noise.</p></li>
<li><p>Set this to a value close to zero (1e-10) if the function is
noise-free. Setting to zero might cause stability issues.</p></li>
</ul>
</p></li>
<li><p><strong>n_jobs</strong><strong> [</strong><strong>int</strong><strong>, </strong><strong>default=1</strong><strong>]</strong> (<em>*</em>) – Number of cores to run in parallel while running the lbfgs
optimizations over the acquisition function. Valid only
when <cite>acq_optimizer</cite> is set to “lbfgs.”
Defaults to 1 core. If <cite>n_jobs=-1</cite>, then number of jobs is set
to number of cores.</p></li>
<li><p><strong>model_queue_size</strong><strong> [</strong><strong>int</strong><strong> or </strong><strong>None</strong><strong>, </strong><strong>default=None</strong><strong>]</strong> (<em>*</em>) – Keeps list of models only as long as the argument given. In the
case of None, the list has no capped length.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The optimization result returned as a OptimizeResult object.
Important attributes are:</p>
<ul class="simple">
<li><p><cite>x</cite> [list]: location of the minimum.</p></li>
<li><p><cite>fun</cite> [float]: function value at the minimum.</p></li>
<li><p><cite>models</cite>: surrogate models used for each iteration.</p></li>
<li><dl class="simple">
<dt><cite>x_iters</cite> [list of lists]: location of function evaluation for each</dt><dd><p>iteration.</p>
</dd>
</dl>
</li>
<li><p><cite>func_vals</cite> [array]: function value for each iteration.</p></li>
<li><p><cite>space</cite> [Space]: the optimization space.</p></li>
<li><p><cite>specs</cite> [dict]`: the call specifications.</p></li>
<li><dl class="simple">
<dt><cite>rng</cite> [RandomState instance]: State of the random state</dt><dd><p>at the end of minimization.</p>
</dd>
</dl>
</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html">http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</a></p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>* <cite>res</cite> [<cite>OptimizeResult</cite>, scipy object]</p>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="optimizer.html" class="btn btn-neutral float-right" title="skopt.Optimizer, ask-and-tell interface" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="API Reference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, The scikit-optimize contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>